<!DOCTYPE html>
<html>
<head>
    <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <meta charset="utf-8">
  <title>CS 180 Project 5 – </title>
  <style>
    body { font-family: sans-serif; max-width: 900px; margin: auto; padding: 20px; }
    h1, h2, h3 { font-weight: 600; }
    img { max-width: 100%; height: auto; border-radius: 8px; }
    .img-row { display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 20px; }
    .img-col { flex: 1 1 250px; text-align: center; }
    .caption { font-size: 0.9rem; color: #555; margin-top: 4px; }
  </style>
</head>
<body>
  <h1>CS 180 Project 5 – Diffusion Models</h1>
  <p><strong>Name:</strong> Mansoor Mamnoon</p>
  <p><strong>Random Seed:</strong> 180</p>

  <h3>Prompt Set Overview</h3>

<p>
  Across all parts of this project, I used a fixed set of text prompts to explore different
  capabilities of diffusion-based image generation, including text-to-image synthesis,
  image-to-image translation, visual anagrams, and hybrid images. All prompts were embedded
  in advance and reused consistently throughout the experiments.
</p>

<h4>Part 0 – DeepFloyd IF: Text-to-Image Generation</h4>

<p>
  For the initial text-to-image experiments, I designed three creative prompts that combine
  multiple visual concepts and styles. These prompts were chosen to test the model’s ability
  to handle complex scenes, unusual object combinations, and detailed descriptive language.
</p>

<ul>
  <li>
    <strong>Mechanical Peacock:</strong>
    “A giant mechanical peacock made of bronze gears and stained glass, opening its tail in the
    middle of Times Square at night.”
  </li>
  <li>
    <strong>Victorian Astronaut:</strong>
    “A Victorian-era astronaut reading a book inside a candle-lit library on the moon.”
  </li>
  <li>
    <strong>Gingerbread Cottage:</strong>
    “A warm, cozy cottage made out of chocolate and gingerbread, with smoke shaped like cinnamon
    rolls drifting from the chimney.”
  </li>
</ul>

<h4>Baseline and Utility Prompts</h4>

<p>
  In addition to the creative prompts above, I repeatedly used a small set of baseline prompts
  for unconditional or weakly conditioned generation. These prompts are listed explicitly
  because they appear throughout multiple sections of the project.
</p>

<ul>
  <li><strong>Generic photo prior:</strong> “a high quality photo”</li>
  <li><strong>Unconditional prompt:</strong> empty string (<code>""</code>)</li>
</ul>

<h4>Part 1.7 – Image-to-Image Translation (SDEdit)</h4>

<p>
  For image-to-image translation, I experimented with both unconditional and text-conditioned
  transformations. The unconditional projection experiments (Parts 1.7.0 and 1.7.1) use the
  generic photo prior. The final text-conditioned transformations in Part 1.7.3 use the
  following prompts:
</p>

<ul>
  <li>
    <strong>Campanile → Rocket Launch Tower:</strong>
    “a futuristic rocket launch tower standing on an alien planet with purple sky”
  </li>
  <li>
    <strong>Beach → Penguins:</strong>
    “a snowy mountain landscape with penguins sliding down icy hills”
  </li>
  <li>
    <strong>Coastal Town → Mushroom Village:</strong>
    “a medieval village built entirely out of enormous glowing mushrooms”
  </li>
</ul>

<h4>Part 1.8 – Visual Anagrams</h4>

<p>
  For the visual anagram experiments, I generated three illusion pairs, where each image can
  be interpreted differently depending on the viewing direction. Each illusion is defined
  by a pair of prompts:
</p>

<ul>
  <li>
    <strong>Illusion 1 (Canonical):</strong><br>
    “an oil painting of an old man”<br>
    “an oil painting of people around a campfire”
  </li>
  <li>
    <strong>Illusion 2:</strong><br>
    “a detailed pencil drawing of a cat”<br>
    “a spooky human skull made of vines”
  </li>
  <li>
    <strong>Illusion 3:</strong><br>
    “a fantasy castle on a hill at sunset”<br>
    “a dragon’s head made of clouds”
  </li>
</ul>

<h4>Part 1.9 – Hybrid Images (Factorized Diffusion)</h4>

<p>
  For hybrid image generation, I combined pairs of prompts by taking low-frequency components
  from one prompt and high-frequency components from another. The strongest hybrid pairs are
  listed below:
</p>

<ul>
  <li>
    “a large stone skull on a dark background”<br>
    “a detailed photo of a waterfall in a lush green forest”
  </li>
  <li>
    “a colorful nebula galaxy with soft glowing clouds”<br>
    “a detailed pencil drawing of an owl’s face”
  </li>
  <li>
    “a soft blurry sunset over the ocean with warm colors”<br>
    “a detailed black and white line drawing of a city skyline”
  </li>
  <li>
    “a soft focus color portrait of a woman smiling”<br>
    “a detailed black and white pencil drawing of a human skull”
  </li>
</ul>

<p>
  In total, this project uses 25 distinct prompts. This set is sufficient to justify precomputing
  prompt embeddings and demonstrates a range of creative, structural, and semantic challenges
  for diffusion-based models.
</p>


  <h2>Part 0: Playing with DeepFloyd IF</h2>

  <p>
    I used the DeepFloyd IF two-stage diffusion model through Hugging Face. Prompt embeddings
    were generated in advance using the provided Hugging Face cluster and loaded locally from
    <code>prompt_embeds_dict.pth</code>. For all experiments in this section, I fixed the random
    seed to <code>180</code> to ensure reproducibility and isolate the effect of the diffusion
    process itself.
  </p>
  
  <h3>Prompts</h3>
  <ol>
    <li>A giant mechanical peacock made of bronze gears and stained glass, opening its tail in the middle of Times Square at night.</li>
    <li>A Victorian-era astronaut reading a book inside a candle-lit library on the moon.</li>
    <li>A warm, cozy cottage made out of chocolate and gingerbread, with smoke shaped like cinnamon rolls drifting from the chimney.</li>
  </ol>
  
  <h3>Generated Images (Stage II, 256×256, 20 vs. 50 Steps)</h3>

  <h4> Random Seed: 180</h4>
  


  <p>
    I first generated baseline images using <strong>20 denoising steps</strong> for all three prompts.
    I then reran the same batch of prompts with <strong>50 denoising steps</strong>, keeping the seed,
    prompt embeddings, and pipeline configuration fixed. This allows a direct comparison of image
    quality as a function of <code>num_inference_steps</code>.
  </p>
  
  <div class="img-row">
    <div class="img-col">
      <img src="img/peacock_50steps.png">
      <div class="caption">Prompt 1 – 50 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/astronaut_50.png">
      <div class="caption">Prompt 2 – 50 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/cottage50.png">
      <div class="caption">Prompt 3 – 50 denoising steps</div>
    </div>
  </div>
  
  <div class="img-row">
    <div class="img-col">
      <img src="img/peacock_20.png">
      <div class="caption">Prompt 1 – 20 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/astronaut_20.png">
      <div class="caption">Prompt 2 – 20 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/cottage_20.png">
      <div class="caption">Prompt 3 – 20 denoising steps</div>
    </div>
  </div>
  
  <h3>Reflection on Image Quality and Denoising Steps</h3>

  <p>
    Comparing images generated with 20 and 50 denoising steps shows that increasing
    <code>num_inference_steps</code> improves image quality and, in some cases, changes how well the
    image matches the prompt. Since the random seed and prompts are fixed, these differences come
    from running the diffusion process for more steps.
  </p>
  
  <p>
    For the mechanical peacock and the gingerbread cottage, the overall scene looks similar at both
    step counts, but the 50-step images are clearer. The peacock’s feathers and stained-glass patterns
    are sharper, and the city lights are cleaner. The gingerbread cottage has more defined icing and
    candy details at 50 steps, although the smoke still does not clearly form cinnamon-roll shapes.
  </p>
  
  <p>
    The astronaut prompt shows a bigger change. With 20 steps, the image looks like a medieval or
    scholarly person reading in a library, with little visual evidence of space. With 50 steps, the
    person clearly appears in an astronaut suit, and the background looks like the moon. This shows
    that using more denoising steps can help the model better combine different ideas in a complex
    prompt.
  </p>
  
  <p>
    Overall, more denoising steps usually make images sharper, and in harder prompts, they can also
    improve how well the image matches the text. This comes at the cost of more computation time.
  </p>
  
  

  <h2>Part 1.1: Forward Diffusion Process</h2>

<p>
  In this section, I visualize the <strong>forward process</strong> of diffusion on a real image.
  Starting from a clean 64×64 Campanile photo \(x_0\), I apply the forward noising equation
  \(
    x_t = \sqrt{\bar{\alpha}_t}\, x_0 + \sqrt{1 - \bar{\alpha}_t}\, \epsilon
  \)
  using the same \( \bar{\alpha}_t \) schedule as the DeepFloyd model. As the timestep
  <code>t</code> increases, the contribution of the original image decreases and the contribution of
  Gaussian noise increases.
</p>

<h3>Implementation: Forward Diffusion Function</h3>

<p>
  I implement the forward diffusion step
  \(x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1 - \bar{\alpha}_t}\,\epsilon\)
  in a helper routine <code>forward(im, t)</code>. Given a clean image tensor \(x_0\) and an integer
  timestep <code>t</code>, the routine:
</p>

<ul>
  <li>Reads the cumulative noise schedule value \(\bar{\alpha}_t\) from the pretrained array <code>alphas_cumprod</code>.</li>
  <li>Computes the scalar weights \(\sqrt{\bar{\alpha}_t}\) and \(\sqrt{1 - \bar{\alpha}_t}\).</li>
  <li>Samples fresh Gaussian noise \(\epsilon \sim \mathcal{N}(0, I)\) with the same shape as the input image.</li>
  <li>Forms the noisy image by taking a weighted combination of the clean image and the sampled noise according to the forward diffusion equation.</li>
  <li>Runs without gradient tracking (inference-only), since this step is only used to generate visualizations.</li>
</ul>

<p>
  The output is a tensor \(x_t\) with the same shape as the input image, representing the image after adding the
  amount of noise prescribed by timestep <code>t</code>.
</p>

<h3>Campanile at Different Noise Levels</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Campanile, clean image (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t250.png" alt="Noisy Campanile at t = 250">
    <div class="caption">Campanile at t = 250</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t500.png" alt="Noisy Campanile at t = 500">
    <div class="caption">Campanile at t = 500</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t750.png" alt="Noisy Campanile at t = 750">
    <div class="caption">Campanile at t = 750</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  At <strong>t = 0</strong>, the image is completely clean: we see a sharp Campanile with clear
  edges and textures. At <strong>t = 250</strong>, there is noticeable grain, but the tower and
  background are still clearly recognizable. By <strong>t = 500</strong>, the structure is much
  harder to see: the noise dominates and only a faint outline of the tower remains. At
  <strong>t = 750</strong>, the image is nearly pure noise, and most semantic content has been
  destroyed. This matches the design of the diffusion forward process: as t increases, the signal
  \(\sqrt{\bar{\alpha}_t} x_0\) shrinks, and the noise term \(\sqrt{1 - \bar{\alpha}_t} \epsilon\)
  grows, providing a smooth path from a real image to pure Gaussian noise.
</p>


  <h2>Part 1.2: Classical Gaussian Denoising</h2>

<p>
  In this part, I tried to denoise the forward-diffused Campanile images from
  \(t = 250, 500, 750\) using a classical Gaussian blur filter. The idea is that
  averaging nearby pixels should reduce high-frequency noise. However, Gaussian
  blur does not know anything about the underlying image structure, so it
  inevitably removes edges and details along with the noise.
</p>

<h3>Noisy vs. Gaussian-Blurred Campanile</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t250.png" alt="Noisy Campanile at t = 250">
    <div class="caption">Noisy Campanile (t = 250)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gauss_t250.png" alt="Gaussian blur denoised Campanile at t = 250">
    <div class="caption">Gaussian blur denoised (t = 250)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t500.png" alt="Noisy Campanile at t = 500">
    <div class="caption">Noisy Campanile (t = 500)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gauss_t500.png" alt="Gaussian blur denoised Campanile at t = 500">
    <div class="caption">Gaussian blur denoised (t = 500)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t750.png" alt="Noisy Campanile at t = 750">
    <div class="caption">Noisy Campanile (t = 750)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gauss_t750.png" alt="Gaussian blur denoised Campanile at t = 750">
    <div class="caption">Gaussian blur denoised (t = 750)</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  For <strong>t = 250</strong>, Gaussian blur slightly smooths the grain and makes the sky look
  cleaner, but it also softens the edges of the Campanile. At <strong>t = 500</strong>, the filter
  removes some high-frequency noise, yet the tower structure becomes very blurry and hard to see.
  By <strong>t = 750</strong>, almost all semantic content has already been destroyed by the forward
  process, and Gaussian blur can only smooth the noise into an even more featureless blob. This
  illustrates a key limitation of classical denoising: simple low-pass filters cannot reconstruct
  lost structure; they can only trade noise for sharpness.
</p>


<h2>Part 1.3: One-Step Denoising with the DeepFloyd UNet</h2>

<p>
  In this part, I use the pretrained DeepFloyd Stage&nbsp;1 UNet to perform
  <strong>one-step denoising</strong> on the noisy Campanile images from
  \(t = 250, 500, 750\). The UNet has been trained on a huge dataset of
  \((x_0, x_t, t)\) pairs to predict the Gaussian noise \(\epsilon\) that was
  added at each timestep. Given a noisy image \(x_t\), a timestep \(t\), and a
  text embedding (here I use the unconditional/null embedding), the model
  predicts \(\hat{\epsilon}\). Using the forward diffusion equation
  \[
    x_t = \sqrt{\bar{\alpha}_t}\, x_0 + \sqrt{1 - \bar{\alpha}_t}\, \epsilon,
  \]
  I invert it to recover an estimate of the original image:
  \[
    \hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t}\, \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}}.
  \]
</p>

<h3>Original vs Noisy vs One-Step Denoised</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t250.png" alt="Noisy Campanile at t = 250">
    <div class="caption">Noisy Campanile (t = 250)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_onestep_t250.png" alt="One-step denoised Campanile at t = 250">
    <div class="caption">One-step denoised (t = 250)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t500.png" alt="Noisy Campanile at t = 500">
    <div class="caption">Noisy Campanile (t = 500)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_onestep_t500.png" alt="One-step denoised Campanile at t = 500">
    <div class="caption">One-step denoised (t = 500)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t750.png" alt="Noisy Campanile at t = 750">
    <div class="caption">Noisy Campanile (t = 750)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_onestep_t750.png" alt="One-step denoised Campanile at t = 750">
    <div class="caption">One-step denoised (t = 750)</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  Compared to the <strong>Gaussian blur</strong> results in Part&nbsp;1.2, the learned UNet
  denoiser produces much sharper and more faithful reconstructions. At
  \(t = 250\), the one-step denoised image is very close to the original Campanile,
  with clear edges and recognizable structure. At \(t = 500\), some details are
  still recovered, but fine textures and contrast begin to wash out. By
  \(t = 750\), the model struggles: the forward process has already destroyed
  most semantic information, so the reconstruction looks more like a generic
  “building-like” blob than the true Campanile.
</p>

<p>
  This highlights a key advantage of diffusion models over classical filters:
  instead of blindly smoothing pixels, the UNet has learned a prior over natural
  images and uses both the noisy input and the timestep \(t\) to infer which
  structures are likely to be real signal and which are noise. However, it also
  shows a limitation of one-step denoising from very large \(t\): when the noise
  level is too high, the information about the original image is fundamentally
  ambiguous, and even a strong generative model cannot perfectly reconstruct it.
</p>

<h2>Part 1.4: Iterative Denoising</h2>

<p>
  In this section, I implement <strong>iterative denoising</strong> with DeepFloyd IF. Instead of
  attempting to recover the clean image <code>x_0</code> in a single step, I begin from a very
  noisy image at timestep <code>t = 690</code> (from a strided schedule
  <code>[990, 960, ..., 0]</code>) and repeatedly apply the DDPM update rule to move toward
  progressively smaller timesteps until reaching <code>t = 0</code>.
</p>

<p>
  At each step, the UNet predicts both the noise component and a learned variance term. I first
  estimate <code>x_0</code> from the current noisy image <code>x_t</code>, use this to construct
  the DDPM mean for the next timestep <code>x_{t'}</code>, and then add the correct amount of
  variance using the provided <code>add_variance</code> function. This produces a sequence of
  images that gradually increase signal-to-noise ratio while remaining consistent with the
  diffusion model’s training process.
</p>

<h3>Implementation: Iterative Denoising with Learned Variance</h3>

<p>
  To reverse the diffusion process, I repeatedly apply a DDPM-style update
  from a noisy image \(x_t\) to a slightly less noisy image \(x_{t'}\).
  The helper <code>add_variance</code> uses the scheduler’s learned variance
  prediction to add the correct amount of noise at each step, and
  <code>iterative_denoise</code> loops over all timesteps until reaching
  \(t = 0\).
</p>

<p>
  I implement this in two helper routines:
</p>

<ul>
  <li>
    <code>add_variance(predicted_variance, t, image)</code> takes the UNet’s variance prediction for the current
    timestep and asks the DDPM scheduler for the corresponding variance value. It then samples fresh Gaussian
    noise with the same shape as the image, scales it according to the scheduler’s variance, and adds it to the
    provided image. This matches the stochasticity used in DDPM sampling.
  </li>
  <li>
    <code>iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display)</code> performs the full reverse
    process starting from a noisy image at <code>timesteps[i_start]</code> and repeatedly stepping to the next
    smaller timestep until reaching <code>t = 0</code>. The loop runs without gradient tracking since sampling is
    inference-only.
  </li>
</ul>

<p>
  Inside the iterative denoising loop, each iteration follows the DDPM sampling structure:
</p>

<ul>
  <li>
    Select the current timestep <code>t</code> and the next timestep <code>t'</code> (the next element in the
    strided schedule).
  </li>
  <li>
    Look up the cumulative alpha products \(\bar{\alpha}_t\) and \(\bar{\alpha}_{t'}\) from
    <code>alphas_cumprod</code>, and compute the corresponding per-step \(\alpha\) and \(\beta\) values that
    relate these two timesteps.
  </li>
  <li>
    Run the stage-1 DeepFloyd UNet conditioned on <code>prompt_embeds</code> to predict (1) the noise estimate
    for the current image and (2) a learned variance representation. The UNet output is split into these two
    components.
  </li>
  <li>
    Use the predicted noise together with \(\bar{\alpha}_t\) to form an estimate \(\hat{x}_0\) of the original
    clean image (the standard DDPM inversion formula).
  </li>
  <li>
    Compute the DDPM mean for the previous timestep \(x_{t'}\) as a weighted combination of \(\hat{x}_0\) and
    the current image \(x_t\), using the closed-form coefficients derived from the diffusion process.
  </li>
  <li>
    Add the appropriate stochasticity by calling <code>add_variance</code> with the UNet’s learned variance term
    for timestep <code>t</code>. This produces the sampled \(x_{t'}\).
  </li>
  <li>
    Move tensors to the scheduler’s expected device/dtype for the variance computation, then move the result
    back to the model’s device/dtype for the next UNet call.
  </li>
  <li>
    If <code>display</code> is enabled, periodically print the step information and visualize the intermediate
    image so the gradual denoising trajectory is observable.
  </li>
</ul>

<p>
  The function returns the final denoised image at timestep <code>t = 0</code> as a numpy array (still in the
  model’s normalized range).
</p>

<h3>Noisy Campanile at Selected Timesteps</h3>

<p>
  Below are snapshots of the Campanile after the forward diffusion process at several timesteps
  in my strided schedule. These correspond to intermediate states that the iterative denoiser
  must reverse:
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_noisy_t690.png" alt="Noisy Campanile at t = 690">
    <div class="caption">Noisy Campanile (t = 690)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_noisy_t540.png" alt="Noisy Campanile at t = 540">
    <div class="caption">Noisy Campanile (t = 540)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_noisy_t390.png" alt="Noisy Campanile at t = 390">
    <div class="caption">Noisy Campanile (t = 390)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_noisy_t240.png" alt="Noisy Campanile at t = 240">
    <div class="caption">Noisy Campanile (t = 240)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_noisy_t90.png" alt="Noisy Campanile at t = 90">
    <div class="caption">Noisy Campanile (t = 90)</div>
  </div>
</div>

<h3>Iterative vs One-Step vs Gaussian Denoising</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean_t0.png" alt="Original Campanile, clean image">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_iter_denoised.png" alt="Iteratively denoised Campanile">
    <div class="caption">Iteratively denoised (start t = 690)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_one_step.png" alt="One-step denoised Campanile">
    <div class="caption">One-step denoised from t = 690</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gaussian_blur.png" alt="Gaussian blurred Campanile">
    <div class="caption">Gaussian blur (kernel 5, σ = 2)</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  Starting from very heavy noise (<code>t = 690</code>), the
  <strong>one-step</strong> denoising estimate actually preserves the
  <em>structure</em> of the Campanile surprisingly well: the tower shape and
  the original top are still recognizable, even though the image is quite
  noisy and low quality overall. This makes sense, because we only apply the
  UNet once to directly invert the noisy sample <code>x_t</code>, without
  repeatedly resampling new noise.
</p>

<p>
  In contrast, the <strong>iterative</strong> denoising procedure produces a
  much cleaner and sharper image, but it sometimes
  <strong>hallucinates a different tower top</strong> than in the original
  photograph. By the time we reach <code>t = 690</code> in the forward
  process, most of the original signal has been destroyed, so the model is
  effectively “dreaming” a plausible tall tower rather than reconstructing
  the exact Campanile. Repeatedly estimating <code>\hat{x}_0</code>,
  resampling, and adding variance allows the sampler to move along the
  diffusion model’s natural image manifold, which improves realism but can
  drift away from the exact ground-truth details.
</p>

<p>
  Finally, the <strong>Gaussian blur</strong> baseline is smooth but
  fundamentally limited: it only removes high-frequency noise and cannot
  recover missing structure. Compared to it, diffusion-based denoising (both
  one-step and iterative) is able to restore meaningful geometry and texture,
  highlighting the advantage of learning a generative model of natural
  images rather than relying on purely classical filtering.
</p>

<hr>


<h2>Part 1.5: Diffusion Model Sampling</h2>

<p>
  In this section, I use my <code>iterative_denoise</code> function not to clean up an existing
  image, but to <strong>generate images from pure noise</strong>. Following the assignment,
  I sample an initial tensor
  \(x_T \sim \mathcal{N}(0, I)\) of shape \(1 \times 3 \times 64 \times 64\) and then run the
  denoising loop from the noisiest timestep down to \(t = 0\) using the same
  <code>strided_timesteps</code> schedule as in Part&nbsp;1.4.
</p>

<p>
  The denoiser is conditioned on the text prompt embedding for
  <code>"a high quality photo"</code>. At each timestep \(t\), the UNet predicts the noise in the
  current image, which I combine with the diffusion coefficients
  \(\bar{\alpha}_t\) to form an estimate of the clean image \(x_0\). I then use the DDPM update
  rule with learned variance to step from \(x_t\) to a slightly less noisy image \(x_{t'}\),
  and repeat until I reach a final sample at \(t = 0\).
</p>

<h3>Samples from Pure Noise (Prompt: "a high quality photo")</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/part1_5_sample_1.png" alt="Sample 1 from pure noise">
    <div class="caption">Sample 1</div>
  </div>
  <div class="img-col">
    <img src="img/part1_5_sample_2.png" alt="Sample 2 from pure noise">
    <div class="caption">Sample 2</div>
  </div>
  <div class="img-col">
    <img src="img/part1_5_sample_3.png" alt="Sample 3 from pure noise">
    <div class="caption">Sample 3</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/part1_5_sample_4.png" alt="Sample 4 from pure noise">
    <div class="caption">Sample 4</div>
  </div>
  <div class="img-col">
    <img src="img/part1_5_sample_5.png" alt="Sample 5 from pure noise">
    <div class="caption">Sample 5</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  All five images start from independent Gaussian noise but share the same text prompt
  <code>"a high quality photo"</code>. Even without classifier-free guidance, the model produces
  reasonably realistic, photo-like outputs: we see natural textures, lighting, and depth,
  rather than pure noise. However, the samples are still somewhat soft or painterly and can
  contain artifacts and odd details. This matches the expectation from the assignment:
  the iterative denoising procedure steers noise towards the natural image manifold, but
  without stronger guidance (which I add later with CFG), the images are only loosely
  constrained by the prompt.
</p>


<h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>

<p>
  In Part 1.5, I sampled images by denoising pure Gaussian noise using my
  iterative diffusion sampler. The results looked vaguely photographic, but
  many images were blurry or lacked clear structure. In this section I use
  <strong>Classifier-Free Guidance (CFG)</strong> to improve sample quality.
</p>

<p>
  At each denoising step, the UNet predicts two noise estimates:
  a <em>conditional</em> noise estimate \(\epsilon_c\) given the text prompt
  "<code>a high quality photo</code>", and an <em>unconditional</em> noise
  estimate \(\epsilon_u\) given the empty prompt "".
  CFG combines them as:
</p>

<p style="text-align:center;">
  \[
    \epsilon = \epsilon_u + \gamma \, (\epsilon_c - \epsilon_u),
  \]
</p>

<p>
  where \(\gamma\) is the guidance scale. I use \(\gamma = 7\). When
  \(\gamma = 0\), the model ignores the text and behaves unconditionally;
  when \(\gamma = 1\), it uses the normal conditional prediction; and when
  \(\gamma &gt; 1\), it exaggerates the difference between conditional and
  unconditional noise, which tends to produce sharper, more on-prompt images
  at the cost of diversity.
</p>

<h3>Implementation: Iterative Denoising with Classifier-Free Guidance</h3>

<p>
  For Classifier-Free Guidance (CFG), I run the UNet twice at each timestep:
  once with the conditional prompt embedding for
  "<code>a high quality photo</code>", and once with the unconditional embedding
  for the empty prompt. I then combine these two noise predictions using
  \(\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)\) with guidance
  scale \(\gamma = 7\), and use the resulting guided noise in the same
  DDPM-style update rule as in Part&nbsp;1.4.
</p>

<p>
  Before sampling, I precompute and store the text embeddings for both the
  conditional prompt ("<code>a high quality photo</code>") and the
  unconditional empty prompt. These embeddings are reused at every timestep
  to avoid recomputing text encodings inside the denoising loop.
</p>

<p>
  The iterative CFG denoising procedure operates as follows:
</p>

<ul>
  <li>
    Initialize the process from a noisy image at the chosen starting timestep
    in the strided schedule.
  </li>
  <li>
    At each iteration, select the current timestep <code>t</code> and the next
    smaller timestep <code>t'</code>.
  </li>
  <li>
    Look up the cumulative noise coefficients \(\bar{\alpha}_t\) and
    \(\bar{\alpha}_{t'}\), and compute the corresponding \(\alpha\) and
    \(\beta\) values that define the DDPM transition between these timesteps.
  </li>
  <li>
    Run the UNet once conditioned on the text prompt to obtain the conditional
    noise prediction \(\epsilon_c\) and a learned variance term.
  </li>
  <li>
    Run the same UNet a second time with the empty prompt to obtain the
    unconditional noise prediction \(\epsilon_u\).
  </li>
  <li>
    Combine the two noise estimates using the classifier-free guidance formula
    \(\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)\), which amplifies
    the directions in noise space that are aligned with the text prompt.
  </li>
  <li>
    Use the guided noise estimate to form an estimate \(\hat{x}_0\) of the clean
    image from the current noisy image \(x_t\).
  </li>
  <li>
    Compute the DDPM mean for the previous timestep \(x_{t'}\) as a weighted
    combination of \(\hat{x}_0\) and \(x_t\).
  </li>
  <li>
    Add the appropriate amount of stochasticity by injecting the scheduler’s
    learned variance, ensuring the sampling process matches the model’s
    training-time noise distribution.
  </li>
  <li>
    Move tensors between the scheduler’s device and the model’s device as
    needed, and optionally visualize intermediate results to monitor the
    denoising trajectory.
  </li>
</ul>

<p>
  The loop continues until reaching <code>t = 0</code>, at which point the final
  output is a denoised image sample in the model’s normalized range.
</p>

<h3>CFG Samples for "a high quality photo"</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/cfg_sample_1.png" alt="CFG sample 1: portrait-style photo">
    <div class="caption">Sample 1 with CFG (\(\gamma = 7\))</div>
  </div>
  <div class="img-col">
    <img src="img/cfg_sample_2.png" alt="CFG sample 2: portrait-style photo">
    <div class="caption">Sample 2 with CFG</div>
  </div>
  <div class="img-col">
    <img src="img/cfg_sample_3.png" alt="CFG sample 3: portrait-style photo">
    <div class="caption">Sample 3 with CFG</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/cfg_sample_4.png" alt="CFG sample 4: portrait-style photo">
    <div class="caption">Sample 4 with CFG</div>
  </div>
  <div class="img-col">
    <img src="img/cfg_sample_5.png" alt="CFG sample 5: portrait-style photo">
    <div class="caption">Sample 5 with CFG</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  All samples use the same random seed setup as earlier parts and are generated
  by starting from pure Gaussian noise and running the
  classifier-free guided iterative denoising procedure with the prompt
  "<code>a high quality photo</code>" and guidance scale \(\gamma = 7\).
  Compared to the unguided samples in Part 1.5, these images are noticeably
  more <strong>photorealistic</strong>: they look like well-lit portraits with
  sharper edges, clearer facial features, and more consistent lighting and
  background structure. At the same time, the samples are less diverse: most
  images collapse toward a similar “high-quality portrait” style, which
  illustrates the usual tradeoff of CFG – higher fidelity but reduced variety.
</p>



<h2>Part 1.7: Image-to-image Translation (SDEdit)</h2>

<p>
  In this part I reuse my iterative denoising sampler, but instead of starting from pure noise
  I start from a <em>noised real image</em>. I first apply the forward diffusion process to the
  original 64×64 Campanile image to obtain \(x_t\) at various timesteps, and then run
  <code>iterative_denoise_cfg</code> from that point down to \(t = 0\) using the prompt
  "<code>a high quality photo</code>". This is essentially the SDEdit algorithm: it projects
  a noisy image back onto the natural image manifold, with the amount of change controlled by
  how much noise I add.
</p>

<h3>1.7 – Campanile SDEdit (noise levels 1, 3, 5, 7, 10, 20)</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_campanile_i1.png" alt="Campanile SDEdit i_start=1">
    <div class="caption">Campanile SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i3.png" alt="Campanile SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i5.png" alt="Campanile SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_campanile_i7.png" alt="Campanile SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i10.png" alt="Campanile SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i20.png" alt="Campanile SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_campanile_original.png" alt="Original Campanile">
    <div class="caption">Original Campanile</div>
  </div>
</div>

<p>
  For very small noise (i_start = 1, 3), the edits are subtle: the sky and trees shift slightly
  but the tower is almost unchanged. As I start from larger noise levels (i_start = 10, 20), the
  model has to hallucinate much more content, so the campanile’s top and background can change
  substantially while still staying “photo-like”.
</p>

<h3>1.7 – SDEdit on My Own Photos (House and Beach)</h3>

<p>
  I also applied the same procedure to two web images: a coastal town with white houses and red
  roofs, and a beach scene with a hat and sandals. In each case I ran SDEdit from noise levels
  [1, 3, 5, 7, 10, 20] with the generic prompt "<code>a high quality photo</code>".
</p>

<h4>Coastal Town</h4>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_i1.png" alt="House SDEdit i_start=1">
    <div class="caption">House SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i3.png" alt="House SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i5.png" alt="House SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_i7.png" alt="House SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i10.png" alt="House SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i20.png" alt="House SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_original.png" alt="Original house photo">
    <div class="caption">Original coastal town photo</div>
  </div>
</div>

<h4>Beach Scene</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_i1.png" alt="Beach SDEdit i_start=1">
    <div class="caption">Beach SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i3.png" alt="Beach SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i5.png" alt="Beach SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_i7.png" alt="Beach SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i10.png" alt="Beach SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i20.png" alt="Beach SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_original.png" alt="Original beach photo">
    <div class="caption">Original beach photo</div>
  </div>
</div>

<h3>1.7.1 Editing Hand-Drawn and Web Images</h3>

<p>
  SDEdit is especially fun when I start from non-realistic inputs such as clipart and my own
  sketches. Below I show one web image (a vector-style cat) and two hand-drawn images that I
  drew in the Colab canvas, each projected to the natural image manifold at increasing noise
  levels.
</p>

<h4>Web Image SDEdit</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_web_i1.png" alt="Web SDEdit i_start=1">
    <div class="caption">Web SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i3.png" alt="Web SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i5.png" alt="Web SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_web_i7.png" alt="Web SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i10.png" alt="Web SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i20.png" alt="Web SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_web_original.png" alt="Original web image">
    <div class="caption">Original web image</div>
  </div>
</div>

<h4>Two Hand-Drawn Inputs</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand1_i1.png" alt="Hand-drawn 1 SDEdit i_start=1">
    <div class="caption">Hand-drawn #1, i_start = 1</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i3.png" alt="Hand-drawn 1 SDEdit i_start=3">
    <div class="caption">Hand-drawn #1, i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i5.png" alt="Hand-drawn 1 SDEdit i_start=5">
    <div class="caption">Hand-drawn #1, i_start = 5</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i7.png" alt="Hand-drawn 1 SDEdit i_start=7">
    <div class="caption">Hand-drawn #1, i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i10.png" alt="Hand-drawn 1 SDEdit i_start=10">
    <div class="caption">Hand-drawn #1, i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i20.png" alt="Hand-drawn 1 SDEdit i_start=20">
    <div class="caption">Hand-drawn #1, i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand1_original.png" alt="Original hand-drawn 1">
    <div class="caption">Original hand-drawn #1</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand2_i1.png" alt="Hand-drawn 2 SDEdit i_start=1">
    <div class="caption">Hand-drawn #2, i_start = 1</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i3.png" alt="Hand-drawn 2 SDEdit i_start=3">
    <div class="caption">Hand-drawn #2, i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i5.png" alt="Hand-drawn 2 SDEdit i_start=5">
    <div class="caption">Hand-drawn #2, i_start = 5</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i7.png" alt="Hand-drawn 2 SDEdit i_start=7">
    <div class="caption">Hand-drawn #2, i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i10.png" alt="Hand-drawn 2 SDEdit i_start=10">
    <div class="caption">Hand-drawn #2, i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i20.png" alt="Hand-drawn 2 SDEdit i_start=20">
    <div class="caption">Hand-drawn #2, i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand2_original.png" alt="Original hand-drawn 2">
    <div class="caption">Original hand-drawn #2</div>
  </div>
</div>

<h3>1.7.2 Inpainting</h3>

<p>
  For inpainting, I modify my CFG denoising loop so that at every timestep I overwrite the
  unmasked region with a noised version of the original image \(x_{\text{orig}}\) at the same
  timestep. If \(m\) is a binary mask where 1 indicates the editable region, the update is
</p>

<p style="text-align:center;">
  \(x_t \leftarrow m \odot x_t + (1 - m)\odot \text{forward}(x_{\text{orig}}, t).\)
</p>

<p>
  This keeps everything outside the mask consistent with the original photo, while the diffusion
  model freely resynthesizes content inside the mask.
</p>

<h4>Implementation: Inpainting Function</h4>

<p>
  The inpainting procedure reuses the classifier-free guided iterative denoising loop from the
  previous section, with one crucial modification: at every diffusion step, pixels outside the
  mask are forced to match the original image corrupted to the same noise level. This ensures
  that only the masked region is edited, while the rest of the image remains faithful to the
  original input.
</p>

<p>
  The inpainting routine operates as follows:
</p>

<ul>
  <li>
    Start from pure Gaussian noise with the same shape as the original image, which initializes
    the editable region without any prior structure.
  </li>
  <li>
    Move all inputs (original image, mask, and text embeddings) to the GPU and ensure they share
    compatible data types for efficient inference.
  </li>
  <li>
    Iterate over the diffusion timesteps from large to small, following the chosen (possibly
    strided) schedule.
  </li>
  <li>
    At the beginning of each timestep, overwrite the unmasked region with a forward-diffused
    version of the original image at the current timestep. This enforces that pixels outside the
    mask exactly track the original image’s noise trajectory.
  </li>
  <li>
    Look up the cumulative noise coefficients \(\bar{\alpha}_t\) and \(\bar{\alpha}_{t'}\) for the
    current and next timesteps, and compute the corresponding DDPM transition coefficients.
  </li>
  <li>
    Run the UNet twice at the current timestep: once conditioned on the text prompt and once with
    the empty prompt, producing conditional and unconditional noise estimates along with a
    learned variance term.
  </li>
  <li>
    Combine the two noise predictions using classifier-free guidance, amplifying the influence
    of the text prompt within the masked region.
  </li>
  <li>
    Use the guided noise estimate to compute an estimate \(\hat{x}_0\) of the clean image and
    then form the DDPM mean for the previous timestep.
  </li>
  <li>
    Add stochasticity using the scheduler’s learned variance, ensuring the reverse process
    remains consistent with the model’s training-time distribution.
  </li>
  <li>
    Carry the resulting image forward to the next timestep and repeat until reaching
    \(t = 0\).
  </li>
</ul>

<p>
  The final output is an inpainted image where pixels outside the mask are preserved from the
  original photo, and pixels inside the mask are newly synthesized in a way that is coherent
  with both the surrounding context and the text prompt.
</p>

<h4>Campanile Inpainting</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/inpaint_campanile_original.png" alt="Campanile original">
    <div class="caption">Original Campanile</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_campanile_mask.png" alt="Campanile inpainting mask">
    <div class="caption">Inpainting mask (top of tower)</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_campanile_result.png" alt="Campanile inpainted">
    <div class="caption">Inpainted Campanile</div>
  </div>
</div>

<p>
  The model fills the top of the tower with a plausible structure that matches the lighting and
  perspective of the original image, even though that region was completely resampled.
</p>

<h4>More Inpainting Examples</h4>

<p>
  Here I show three more masks on two different images. Each triplet shows the original image,
  the binary mask, and the final inpainted result.
</p>

<h5>House – Circular Roof Mask</h5>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_original.png" alt="House original">
    <div class="caption">Original house</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_circle_mask.png" alt="House circular mask">
    <div class="caption">Circular roof mask</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_circle_result.png" alt="House circular inpaint">
    <div class="caption">Inpainted house (circular)</div>
  </div>
</div>

<h5>House – Diamond Roof Mask</h5>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_original.png" alt="House original">
    <div class="caption">Original house</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_diamond_mask.png" alt="House diamond mask">
    <div class="caption">Diamond-shaped roof mask</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_diamond_result.png" alt="House diamond inpaint">
    <div class="caption">Inpainted house (diamond)</div>
  </div>
</div>

<h5>Beach – Vertical Band Mask</h5>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_original.png" alt="Beach original">
    <div class="caption">Original beach</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_beach_vertical_mask.png" alt="Beach vertical mask">
    <div class="caption">Vertical band mask</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_beach_vertical_result.png" alt="Beach vertical inpaint">
    <div class="caption">Inpainted beach (vertical band)</div>
  </div>
</div>



<!-- ============================= -->
<!--       1.7.3 SDEdit Section    -->
<!-- ============================= -->

<section id="sedit">
  <h1>1.7.3 Text-Conditioned Image-to-Image Translation</h1>

  <p>
    In this section, I use SDEdit with Classifier-Free Guidance to transform real images
    toward imaginative text prompts. Each transformation begins by adding noise at a given level
    (<code>i_start ∈ {1,3,5,7,10,20}</code>) and then denoising using a text prompt.
    Lower noise levels preserve the original structure, while higher noise levels result
    in stronger transformations.
  </p>

  <!-- ------------------------ -->
  <!-- CAMPANILE EXAMPLE        -->
  <!-- ------------------------ -->
  <h2>Campanile → “Futuristic Rocket Launch Tower”</h2>
  <p>
    <strong>Prompt:</strong>
    <em>a futuristic rocket launch tower standing on an alien planet with purple sky</em>
  </p>

  <div class="image-grid">
    <figure>
      <img src="img/campanile_sedit_rocket_1.png" alt="Campanile i_start 1">
      <figcaption>i_start = 1</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_3.png" alt="Campanile i_start 3">
      <figcaption>i_start = 3</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_5.png" alt="Campanile i_start 5">
      <figcaption>i_start = 5</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_7.png" alt="Campanile i_start 7">
      <figcaption>i_start = 7</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_10.png" alt="Campanile i_start 10">
      <figcaption>i_start = 10</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_20.png" alt="Campanile i_start 20">
      <figcaption>i_start = 20</figcaption>
    </figure>
  </div>

  <figure>
    <img src="img/campanile_clean.png" alt="Original Campanile" class="original-img">
    <figcaption>Original Campanile Image</figcaption>
  </figure>



  <!-- ------------------------ -->
  <!-- BEACH EXAMPLE            -->
  <!-- ------------------------ -->
  <h2>Beach → “Penguins Sliding Down Icy Hills”</h2>
  <p>
    <strong>Prompt:</strong>
    <em>a snowy mountain landscape with penguins sliding down icy hills</em>
  </p>

  <div class="image-grid">
    <figure>
      <img src="img/beach_sedit_penguins_1.png">
      <figcaption>i_start = 1</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_3.png">
      <figcaption>i_start = 3</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_5.png">
      <figcaption>i_start = 5</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_7.png">
      <figcaption>i_start = 7</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_10.png">
      <figcaption>i_start = 10</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_20.png">
      <figcaption>i_start = 20</figcaption>
    </figure>
  </div>

  <figure>
    <img src="img/sedit_beach_original.png" class="original-img">
    <figcaption>Original Beach Image</figcaption>
  </figure>



  <!-- ------------------------ -->
  <!-- HOUSE EXAMPLE            -->
  <!-- ------------------------ -->
  <h2>Coastal Town → “Glowing Mushroom Village”</h2>
  <p>
    <strong>Prompt:</strong>
    <em>a medieval village built entirely out of enormous glowing mushrooms</em>
  </p>

  <div class="image-grid">
    <figure>
      <img src="img/house_sedit_mushrooms_1.png">
      <figcaption>i_start = 1</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_3.png">
      <figcaption>i_start = 3</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_5.png">
      <figcaption>i_start = 5</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_7.png">
      <figcaption>i_start = 7</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_10.png">
      <figcaption>i_start = 10</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_20.png">
      <figcaption>i_start = 20</figcaption>
    </figure>
  </div>

  <figure>
    <img src="img/sedit_house_original.png" class="original-img">
    <figcaption>Original House Image</figcaption>
  </figure>

</section>


<!-- ============================= -->
<!-- CSS for Grid Layout           -->
<!-- ============================= -->

<style>
  #sedit {
    font-family: Arial, sans-serif;
    margin-bottom: 60px;
  }

  .image-grid {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
    gap: 18px;
    margin: 20px 0;
  }

  .image-grid figure {
    text-align: center;
  }

  .image-grid img {
    width: 100%;
    border-radius: 6px;
    border: 1px solid #ccc;
  }

  .original-img {
    width: 40%;
    display: block;
    margin: 25px auto;
    border-radius: 6px;
    border: 1px solid #ccc;
  }

  figcaption {
    margin-top: 6px;
    font-size: 0.9rem;
    color: #555;
  }
</style>


<h2>Part 1.8: Visual Anagrams</h2>

<p>
  In this part I implement <strong>visual anagrams</strong>: single images that look like one
  subject when upright but reveal a different subject when flipped upside down. At each
  denoising step I compute two guided noise estimates:
</p>

<p style="text-align:center;">
  \(\epsilon_1 = \text{CFG}(\text{UNet}(x_t, t, p_1))\) <br>
  \(\epsilon_2 = \text{flip}\big(\text{CFG}(\text{UNet}(\text{flip}(x_t), t, p_2))\big)\)
</p>

<p>
  where \(p_1\) and \(p_2\) are two different prompts, and <code>flip</code> is a vertical flip.
  I then average them:
</p>

<p style="text-align:center;">
  \(\epsilon = (\epsilon_1 + \epsilon_2) / 2\),
</p>

<p>
  and use \(\epsilon\) in the DDPM update. The same underlying latents are constrained to be
  compatible with both prompts, but in opposite orientations.
</p>

<h4>Implementation: Visual Anagram Sampler</h4>

<p>
  The visual anagram sampler is a modified classifier-free guided denoising loop that enforces
  two different semantic interpretations on the same latent image. Instead of producing a
  single guided noise estimate at each timestep, the sampler computes two guided predictions
  corresponding to two prompts and two orientations of the image.
</p>

<p>
  The procedure works as follows:
</p>

<ul>
  <li>
    Initialize the latent image from pure Gaussian noise.
  </li>
  <li>
    Move both prompt embeddings and the unconditional embedding to the GPU and ensure they share
    the same precision as the latent image.
  </li>
  <li>
    Iterate over the diffusion timesteps from large to small, following the chosen schedule.
  </li>
  <li>
    For the first denoising path, run the UNet on the current (upright) latent image conditioned
    on prompt \(p_1\), and also run an unconditional UNet pass. Combine these two predictions
    using classifier-free guidance to obtain a guided noise estimate \(\epsilon_1\).
  </li>
  <li>
    For the second denoising path, vertically flip the current latent image and run the UNet on
    this flipped version conditioned on prompt \(p_2\), along with a corresponding unconditional
    pass. Apply classifier-free guidance to obtain a second guided noise estimate, then flip this
    noise field back to the original orientation to produce \(\epsilon_2\).
  </li>
  <li>
    Average the two guided noise estimates, \(\epsilon_1\) and \(\epsilon_2\), to obtain a single
    noise direction that is simultaneously compatible with both prompts.
  </li>
  <li>
    Use this averaged noise estimate in the standard DDPM update rule to estimate the clean
    image \(\hat{x}_0\), compute the mean of the previous timestep, and add the appropriate amount
    of learned variance via the scheduler.
  </li>
  <li>
    Repeat this process until reaching timestep \(t = 0\).
  </li>
</ul>

<p>
  The final output is a single image whose latent representation has been shaped so that it
  aligns with prompt \(p_1\) when viewed upright and with prompt \(p_2\) when viewed upside down.
</p>

<h3>Old Man / Campfire Illusion</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/anagram_oldman_upright.png" alt="Anagram old man upright">
    <div class="caption">Upright: "an oil painting of an old man"</div>
  </div>
  <div class="img-col">
    <img src="img/anagram_oldman_flipped.png" alt="Anagram campfire flipped">
    <div class="caption">Flipped: "an oil painting of people around a campfire"</div>
  </div>
</div>

<p>
  Upright, the image reads as a portrait of an older man with strong facial shadows. When flipped,
  those same shadow structures reorganize into a ring of figures surrounding a bright central
  glow, resembling a campfire scene.
</p>

<h3>Cat Drawing / Vine Skull Illusion</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/anagram_cat_upright.png" alt="Anagram cat upright">
    <div class="caption">Upright: "a detailed pencil drawing of a cat"</div>
  </div>
  <div class="img-col">
    <img src="img/anagram_cat_flipped.png" alt="Anagram skull flipped">
    <div class="caption">Flipped: "a spooky human skull made of vines"</div>
  </div>
</div>

<p>
  In this illusion, fine details such as fur texture and whiskers in the upright cat transform
  into vine-like strands and hollow eye sockets when flipped, forming the skull structure.
</p>

<h3>Castle / Cloud Dragon Illusion</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/anagram_castle_upright.png" alt="Anagram castle upright">
    <div class="caption">Upright: "a fantasy castle on a hill at sunset"</div>
  </div>
  <div class="img-col">
    <img src="img/anagram_castle_flipped.png" alt="Anagram cloud dragon flipped">
    <div class="caption">Flipped: "a dragon's head made of clouds"</div>
  </div>
</div>

<p>
  Here the silhouette of the castle and surrounding clouds doubles as the jawline, horns, and
  facial contours of a dragon when viewed upside down. In all three cases, the shared latent
  image must satisfy two different prompts simultaneously, which is made possible by the
  dual-path, CFG-based denoising process.
</p>


<h2>Part 1.9: Hybrid Images with Factorized Diffusion</h2>

<p>
  Finally, I implement <strong>hybrid images</strong> using Factorized Diffusion. At each
  timestep I compute two guided noise predictions:
</p>

<p style="text-align:center;">
  \(\epsilon_1 = \text{CFG}(\text{UNet}(x_t, t, p_1)),\quad
    \epsilon_2 = \text{CFG}(\text{UNet}(x_t, t, p_2))\).
</p>

<p>
  I then take low frequencies from \(\epsilon_1\) and high frequencies from \(\epsilon_2\):
</p>

<p style="text-align:center;">
  \(\epsilon = f_{\text{lowpass}}(\epsilon_1) + f_{\text{highpass}}(\epsilon_2),\)
</p>

<p>
  where <code>f_lowpass</code> is implemented with a Gaussian blur (kernel size 33, σ = 2) and
  <code>f_highpass(x) = x - f_lowpass(x)</code>. This composite noise drives the denoising
  process so that the final image looks like prompt \(p_1\) when blurred or viewed from far away,
  but like prompt \(p_2\) when viewed up close.
</p>

<h4>Implementation: Hybrid Image Sampler</h4>

<p>
  The hybrid image sampler extends classifier-free guided diffusion by factorizing the noise
  prediction into low- and high-frequency components coming from two different text prompts.
  Instead of committing to a single semantic interpretation at all spatial scales, the sampler
  allows one prompt to control global structure while another controls fine details.
</p>

<p>
  The procedure operates as follows:
</p>

<ul>
  <li>
    Initialize the latent image from pure Gaussian noise and configure the diffusion scheduler
    with the desired timestep sequence.
  </li>
  <li>
    Move both conditional prompt embeddings and the unconditional embedding to the GPU and ensure
    they share the same precision as the latent image.
  </li>
  <li>
    At each diffusion timestep, run the UNet three times on the current latent image:
    once conditioned on the first prompt \(p_1\), once conditioned on the second prompt \(p_2\),
    and once with the empty prompt to obtain the unconditional prediction.
  </li>
  <li>
    Apply classifier-free guidance separately for each prompt, producing two guided noise
    estimates \(\epsilon_1\) and \(\epsilon_2\) that emphasize their respective semantics.
  </li>
  <li>
    Convert the guided noise estimates to floating-point precision suitable for filtering and
    apply a Gaussian blur to extract the low-frequency component of \(\epsilon_1\).
  </li>
  <li>
    Apply the same Gaussian blur to \(\epsilon_2\) and subtract it from \(\epsilon_2\) to obtain
    the corresponding high-frequency component.
  </li>
  <li>
    Combine these components by adding the low-frequency noise from prompt \(p_1\) to the
    high-frequency noise from prompt \(p_2\), forming a single composite noise field.
  </li>
  <li>
    Optionally renormalize the composite noise so that its variance remains comparable to the
    original guided noises, preventing instability during sampling.
  </li>
  <li>
    Use this hybrid noise estimate in the standard DDPM update rule to estimate the clean image,
    compute the mean of the previous timestep, and add learned variance via the scheduler.
  </li>
  <li>
    Repeat this process until reaching timestep \(t = 0\).
  </li>
</ul>

<p>
  The final output is a hybrid image whose global appearance is dominated by prompt \(p_1\),
  while its fine-scale edges and textures are dominated by prompt \(p_2\).
</p>

<h3>Hybrid Image Results</h3>

<p>
  Below are all of the hybrid images I generated using the factorized diffusion
  sampler. In each case, the low frequencies come from a “soft” or blurry prompt
  (background / global structure), while the high frequencies come from a detailed,
  high-contrast prompt (edges and fine details).
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/hybrid_skull_waterfall.png" alt="Hybrid of stone skull and waterfall">
    <div class="caption">
      Hybrid 1 – Low frequencies: “a detailed photo of a waterfall in a lush green forest”;
      High frequencies: “a large stone skull on a dark background”.
    </div>
  </div>
  <div class="img-col">
    <img src="img/hybrid_nebula_owl.png" alt="Hybrid of nebula galaxy and owl drawing">
    <div class="caption">
      Hybrid 2 – Low frequencies: “a colorful nebula galaxy with soft glowing clouds”;
      High frequencies: “a detailed pencil drawing of an owl’s face”.
    </div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/hybrid_sunset_city.png" alt="Hybrid of sunset and city skyline drawing">
    <div class="caption">
      Hybrid 3 – Low frequencies: “a soft blurry sunset over the ocean with warm colors”;
      High frequencies: “a detailed black and white line drawing of a city skyline”.
    </div>
  </div>
  <div class="img-col">
    <img src="img/hybrid_face_skull.png" alt="Hybrid of portrait and skull drawing">
    <div class="caption">
      Hybrid 4 – Low frequencies: “a soft focus color portrait of a woman smiling”;
      High frequencies: “a detailed black and white pencil drawing of a human skull”.
    </div>
  </div>
</div>

<p>
  At low spatial frequencies the image is dominated by smooth glowing nebula blobs, so it looks
  like a colorful galaxy. At high spatial frequencies, the pencil-like lines that form the owl’s
  eyes and feathers become visible. The hybrid effectively hides a sketch inside a soft
  astronomical background.
</p>

<p>
  Overall, visual anagrams and hybrid images highlight how flexible the diffusion framework is:
  by manipulating the noise estimates at each step (e.g., flipping, frequency splitting), I can
  steer the model to satisfy multiple competing constraints at different spatial scales or
  orientations.
</p>



<h2>Part 2: MNIST Denoising with an Unconditional UNet (Sections 1.2.1–1.2.3)</h2>

<p>
  In this part, I trained an <strong>unconditional UNet</strong> to act as a denoiser on MNIST.
  The denoising objective is mean-squared error:
  \[
    \mathcal{L} = \mathbb{E}_{x,\epsilon}\left[\lVert D_\theta(z) - x \rVert_2^2\right],
    \quad z = x + \sigma \epsilon,\ \epsilon \sim \mathcal{N}(0, I).
  \]
  During training, I sample fresh noise every time a batch is fetched so the network sees new noisy
  inputs each epoch.
</p>

<h3>1.2: Visualizing the Noising Process</h3>
<p>
  I first verified the corruption process by taking a fixed MNIST digit and adding Gaussian noise
  at multiple noise levels \(\sigma \in \{0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0\}\). As \(\sigma\) increases,
  the digit becomes progressively harder to recognize.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/mnist_noising_sigmas1.png" alt="MNIST noising process across sigmas">
    <div class="caption">Noising visualization across \(\sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\).</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> As the noise level increases, the image transitions smoothly from a clean,
  easily recognizable digit to near-pure Gaussian noise. At small \(\sigma\), the digit structure is largely
  preserved with only mild pixel-level corruption. As \(\sigma\) grows, fine details are progressively destroyed,
  and by \(\sigma = 1.0\) the input contains almost no visible information about the underlying digit.
  This visualization confirms that the corruption process behaves as expected and produces a continuous
  spectrum of difficulty for the denoising task.
</p>


<p><strong>Deliverable:</strong> Visualization of the noising process (above).</p>

<hr>

<h3>1.2.1 Training a Denoiser (\(\sigma = 0.5\))</h3>
<p>
  I trained the UNet with hidden dimension \(D=128\), batch size 256, Adam optimizer (lr \(=10^{-4}\)),
  for 5 epochs. For each batch \(x\), I generated \(z = x + 0.5\epsilon\) with fresh \(\epsilon \sim \mathcal{N}(0,I)\),
  and minimized \(\lVert D_\theta(z) - x\rVert_2^2\).
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/mnist_train_loss_sigma0p5.png" alt="Training loss curve sigma 0.5">
    <div class="caption">Training loss curve for \(\sigma = 0.5\).</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/mnist_denoise_epoch1_sigma0p5.png" alt="Denoising after epoch 1 sigma 0.5">
    <div class="caption">Test results after epoch 1 (Clean / Noisy / Denoised), \(\sigma=0.5\).</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/mnist_denoise_epoch5_sigma0p5.png" alt="Denoising after epoch 5 sigma 0.5">
    <div class="caption">Test results after epoch 5 (Clean / Noisy / Denoised), \(\sigma=0.5\).</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> The training loss decreases steadily over epochs, indicating stable optimization.
  After just one epoch, the denoiser already recovers the coarse digit structure, although outputs are blurry
  and lack sharp edges. By epoch 5, the digits are significantly cleaner and closely resemble the ground-truth
  MNIST images. This suggests that even a simple one-step denoising objective is effective when the noise level
  matches the training distribution.
</p>

<p><strong>Deliverables:</strong></p>
<ul>
  <li>Training loss curve plot over training for \(\sigma=0.5\) (above).</li>
  <li>Sample denoising results after the 1st and 5th epoch (above).</li>
</ul>

<hr>

<h3>1.2.2 Out-of-Distribution Testing</h3>
<p>
  After training on \(\sigma=0.5\), I evaluated the same trained model on a <em>fixed</em> test image
  while varying \(\sigma \in [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]\). This checks whether a denoiser trained
  at one noise level generalizes to unseen noise strengths.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/mnist_ood_grid.png" alt="OOD denoising grid across sigmas">
    <div class="caption">OOD denoising results using the same test digit while varying \(\sigma\).</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> Despite being trained only at \(\sigma = 0.5\), the denoiser generalizes well
  to unseen noise levels. For lower noise values, the model nearly perfectly reconstructs the digit.
  As \(\sigma\) increases beyond the training value, the outputs gradually lose fine details and become blurrier,
  but the overall digit identity remains recognizable even at very high noise levels.
  This indicates that the learned denoising function is reasonably robust to moderate distribution shift
  in noise magnitude.
</p>


<p><strong>Deliverable:</strong> Sample results on the test set with out-of-distribution noise levels (above).</p>

<hr>

<h3>1.2.3 Denoising Pure Noise (Generative Behavior)</h3>
<p>
  To make denoising behave like a generative task, I trained a denoiser that maps <em>pure noise</em>
  \(z \sim \mathcal{N}(0, I)\) directly to a clean MNIST image \(x\). Concretely, I fed random noise into the
  same UNet and trained it with MSE to predict a randomly sampled MNIST training digit.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/pure_noise_train_loss.png" alt="Pure noise training loss">
    <div class="caption">Training loss curve for pure-noise-to-MNIST training.</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/pure_noise_epoch1_samples.png" alt="Pure noise samples after epoch 1">
    <div class="caption">Samples after epoch 1 (Input noise / Output).</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/pure_noise_epoch5_samples.png" alt="Pure noise samples after epoch 5">
    <div class="caption">Samples after epoch 5 (Input noise / Output).</div>
  </div>
</div>

<p>
  <strong>Observed pattern:</strong> the outputs tend to look like a <em>blurry “average digit”</em> rather than
  crisp, diverse digits. This happens because with an MSE objective and an input \(z\) that contains no information
  about which digit to output, the best the network can do is predict the <em>conditional mean</em> of the training
  distribution. In other words, it collapses toward a centroid-like image that minimizes average squared error
  across many possible targets (digits 0–9), producing a washed-out prototype.
</p>


<hr>

<h2>Part 2: Training a Flow Matching Model (2.1–2.6)</h2>

<p>
  In Part 1, one-step denoising worked only when the noise level was not too large. In Part 2,
  I implemented <strong>flow matching</strong>, which denoises iteratively by learning a vector field that
  moves samples from pure noise toward the data distribution.
</p>

<p style="text-align:center;">
  \[
    x_t = (1-t)\,x_0 + t\,x_1,\qquad t\in[0,1]
  \]
  \[
    u_t(x_t) = \frac{d x_t}{dt} = x_1 - x_0
  \]
</p>

<p>
  I trained a UNet \(u_\theta(x_t,t)\) (and later \(u_\theta(x_t,t,c)\)) to predict this flow using MSE.
  At sampling time, I start from \(x_0 \sim \mathcal{N}(0,I)\) and apply Euler steps:
</p>

<p style="text-align:center;">
  \[
    x \leftarrow x + \frac{1}{T}\,u_\theta(x,t)
  \]
</p>

<!-- ===================== -->
<!-- 2.1 TIME CONDITIONING -->
<!-- ===================== -->
<h2>2.1 Adding Time Conditioning to the UNet</h2>

<p>
  To condition the UNet on the scalar timestep \(t\), I injected \(t\) using two fully-connected
  blocks (FCBlocks). The timestep is normalized to \([0,1]\), passed through FCBlocks, and then
  used to modulate intermediate UNet activations (following the assignment diagram). Concretely,
  the first time embedding modulates the <code>unflatten</code> features, and the second time embedding
  modulates the <code>up1</code> features.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/part2_1_cond_unet_diagram.png" alt="Figure 4: Conditioned UNet diagram">
    <div class="caption"><strong>Figure 4:</strong> Conditioned UNet architecture used for time conditioning.</div>
  </div>
  <div class="img-col">
    <img src="img/part2_1_fcblock_diagram.png" alt="Figure 5: FCBlock for conditioning">
    <div class="caption"><strong>Figure 5:</strong> FCBlock used to embed conditioning signals (here \(t\)).</div>
  </div>
</div>

<p>
  Since \(t\) is a scalar, I treat it as a 1D input feature (\(F_{in}=1\)) to each FCBlock. The FCBlock output
  is reshaped/broadcast to match the spatial feature map dimensions so it can modulate UNet activations by
  elementwise multiplication at the specified locations.
</p>

<p><strong>Deliverable:</strong> None (implementation step for later training/sampling).</p>

<!-- ================= -->
<!-- 2.2 TRAIN (TIME)  -->
<!-- ================= -->
<h2>2.2 Training the Time-Conditioned UNet</h2>

<p>
  I trained a time-conditioned UNet on MNIST. For each batch \(x_1\), I sampled a random timestep
  \(t \sim \mathcal{U}[0,1]\), constructed an interpolated noisy sample \(x_t\), and trained the model
  to predict the flow \(u_t = x_1 - x_0\) using an MSE objective.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/part2_2_time_unet_train_loss.png" alt="Time-conditioned UNet training loss">
    <div class="caption">Time-conditioned UNet training loss (flow matching), full training trajectory.</div>
  </div>
</div>
<p>
  <strong>Observation:</strong> The training loss decreases smoothly over time, suggesting that the
  time-conditioned UNet successfully learns a consistent flow field across different timesteps.
  Because the model is trained on randomly sampled \(t \in [0,1]\), it must learn to handle both
  highly noisy inputs and nearly clean images within a single network, which this loss curve indicates
  it can do reliably.
</p>


<p><strong>Deliverable:</strong> Training loss curve plot for the time-conditioned UNet (above).</p>

<!-- ================= -->
<!-- 2.3 SAMPLE (TIME) -->
<!-- ================= -->
<h2>2.3 Sampling from the Time-Conditioned UNet</h2>

<p>
  After training, I generated samples by starting from Gaussian noise and iteratively applying the learned
  flow field using Euler integration with \(T\) steps. Over training epochs, samples become progressively more
  legible.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/part2_3_time_samples_epoch1.png" alt="Time-conditioned samples epoch 1">
    <div class="caption">Time-conditioned samples after epoch 1.</div>
  </div>
  <div class="img-col">
    <img src="img/part2_3_time_samples_epoch5.png" alt="Time-conditioned samples epoch 5">
    <div class="caption">Time-conditioned samples after epoch 5.</div>
  </div>
  <div class="img-col">
    <img src="img/part2_3_time_samples_epoch10.png" alt="Time-conditioned samples epoch 10">
    <div class="caption">Time-conditioned samples after epoch 10.</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> Early in training, samples are highly noisy and fragmented, with only weak
  hints of digit-like structure. As training progresses, coherent digit shapes begin to emerge.
  By epoch 5, most samples are recognizable, though some remain blurry or incomplete.
  By epoch 10, digits are substantially sharper and more consistent, demonstrating that iterative flow-based
  denoising is capable of producing high-quality samples from pure noise.
</p>


<p><strong>Deliverable:</strong> Sampling results for epochs 1, 5, and 10 (above).</p>

<!-- ====================== -->
<!-- 2.4 CLASS CONDITIONING -->
<!-- ====================== -->
<h2>2.4 Adding Class Conditioning to the UNet</h2>

<p>
  To improve sample quality and enable controlled generation, I added class conditioning \(c\) (digit label 0–9).
  I represent \(c\) as a one-hot vector and inject it via additional FCBlocks. To support classifier-free guidance,
  I apply class-conditioning dropout: with probability \(p_{\text{uncond}}=0.1\), I set the class vector to 0 so
  the model also learns an unconditional score/flow.
</p>

<p><strong>Deliverable:</strong> None (implementation step for later training/sampling).</p>

<!-- ===================== -->
<!-- 2.5 TRAIN (CLASS-COND) -->
<!-- ===================== -->
<h2>2.5 Training the Class-Conditioned UNet</h2>

<p>
  Training is the same as the time-conditioned case, except the model also takes the class label \(c\).
  During training I periodically generated <strong>unconditional</strong> samples (guidance scale \(=0\)) to confirm
  that the model learned both conditional and unconditional behaviors due to conditioning dropout.
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/part2_5_class_unet_train_loss.png" alt="Class-conditioned UNet training loss">
    <div class="caption">Class-conditioned UNet training loss (flow matching), full training trajectory.</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/part2_5_uncond_epoch1.png" alt="Unconditional samples epoch 1">
    <div class="caption">Unconditional samples (epoch 1).</div>
  </div>
  <div class="img-col">
    <img src="img/part2_5_uncond_epoch5.png" alt="Unconditional samples epoch 5">
    <div class="caption">Unconditional samples (epoch 5).</div>
  </div>
  <div class="img-col">
    <img src="img/part2_5_uncond_epoch10.png" alt="Unconditional samples epoch 10">
    <div class="caption">Unconditional samples (epoch 10).</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> The training loss shows stable convergence similar to the time-only model.
  The unconditional samples generated during training improve steadily, confirming that conditioning dropout
  successfully teaches the model both conditional and unconditional behaviors.
  This is critical for classifier-free guidance, since sampling relies on a meaningful unconditional prediction
  as a reference.
</p>


<p><strong>Deliverable:</strong> Training loss curve plot for the class-conditioned UNet (above).</p>

<h2>Part 2.6: Sampling from the Class-Conditioned UNet (Classifier-Free Guidance)</h2>

<p>
  <strong>Deliverable:</strong> Sampling results from the class-conditioned UNet for epochs
  <strong>1, 5, and 10</strong>, generating <strong>4 instances of each digit (0–9)</strong>.
  I use <strong>classifier-free guidance</strong> with guidance scale \(\gamma = 5.0\).
</p>

<p>
  Following Algorithm B.4, I start from pure Gaussian noise \(x_0 \sim \mathcal{N}(0, I)\)
  and iteratively update:
</p>

<p style="text-align:center;">
  \[
    u = u_{\text{uncond}} + \gamma (u_{\text{cond}} - u_{\text{uncond}}),
    \qquad
    x \leftarrow x + \frac{1}{T} u
  \]
</p>

<p>
  Here \(u_{\text{cond}} = u_\theta(x_t, t, c)\) uses the one-hot digit class \(c\), while
  \(u_{\text{uncond}} = u_\theta(x_t, t, 0)\) drops the class conditioning (the “unconditional” path).
  Increasing \(\gamma\) makes samples more strongly match the requested class, usually at the cost of diversity.
</p>

<hr>

<h3>Sampling Results (With Exponential LR Scheduler)</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/classcond_sched_epoch1_grid.png" alt="Class-conditioned sampling epoch 1 with scheduler">
    <div class="caption">Epoch 1 (with scheduler), \(\gamma = 5.0\)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/classcond_sched_epoch5_grid.png" alt="Class-conditioned sampling epoch 5 with scheduler">
    <div class="caption">Epoch 5 (with scheduler), \(\gamma = 5.0\)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/classcond_sched_epoch10_grid.png" alt="Class-conditioned sampling epoch 10 with scheduler">
    <div class="caption">Epoch 10 (with scheduler), \(\gamma = 5.0\)</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> With the scheduler, class structure becomes recognizable very quickly.
  By epoch 1, many digits are already legible, but there are still small artifacts and some strokes look noisy.
  By epoch 5, digits become noticeably cleaner and more consistent within each class (e.g., 1s are thin and vertical,
  0s are closed loops, 8s show two lobes). By epoch 10, most samples are sharp and strongly class-aligned.
</p>

<p>
  In this run, the learning rate decays each epoch (e.g., from 0.01 down toward 0.001 by epoch 10),
  which stabilizes late training and helps refine details rather than continuing to make large parameter updates.
</p>

<hr>

<h3>Sampling Results (Without Scheduler)</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/classcond_nosched_epoch1_grid.png" alt="Class-conditioned sampling epoch 1 without scheduler">
    <div class="caption">Epoch 1 (no scheduler), \(\gamma = 5.0\)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/classcond_nosched_epoch5_grid.png" alt="Class-conditioned sampling epoch 5 without scheduler">
    <div class="caption">Epoch 5 (no scheduler), \(\gamma = 5.0\)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/classcond_nosched_epoch10_grid.png" alt="Class-conditioned sampling epoch 10 without scheduler">
    <div class="caption">Epoch 10 (no scheduler), \(\gamma = 5.0\)</div>
  </div>
</div>

<p>
  <strong>Observation:</strong> Without the scheduler, the model still converges to strong samples.
  From epoch 1 to epoch 5, digit shapes rapidly become readable and class-correct. From epoch 5 to epoch 10,
  most improvements are incremental (cleaner edges, fewer speckles, more uniform stroke thickness).
  Visually, the final quality is comparable to the scheduled run.
</p>

<hr>

<h3>Removing the Scheduler: What I Changed and Why It Still Works</h3>

<p>
  The assignment asks to remove the exponential learning-rate scheduler while maintaining performance.
  In my “no scheduler” run, I trained with the same optimizer (Adam) and kept a constant learning rate of
  \(1\times10^{-2}\) for all 10 epochs. Empirically, this still converged well for MNIST because the task is
  relatively simple and the class-conditioning signal makes optimization easier (the model does not have to
  represent all digits with a single unconditional mode).
</p>

<p>
  Comparing the two runs, the scheduler mainly helps by shrinking the learning rate late in training so updates
  become more fine-grained. However, even without decay, the model reaches a similar fixed point by epoch 10.
  The main “compensation” here is simply training long enough (10 epochs) with a stable optimizer (Adam) and
  keeping the rest of the setup unchanged (same UNet capacity \(D=64\), same classifier-free guidance setup,
  same timestep discretization \(T=300\), and the same class-dropout probability during training).
</p>











</body>
</html>
