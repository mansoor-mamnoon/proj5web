<!DOCTYPE html>
<html>
<head>
    <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <meta charset="utf-8">
  <title>CS 180 Project 5A – Diffusion Models</title>
  <style>
    body { font-family: sans-serif; max-width: 900px; margin: auto; padding: 20px; }
    h1, h2, h3 { font-weight: 600; }
    img { max-width: 100%; height: auto; border-radius: 8px; }
    .img-row { display: flex; gap: 20px; flex-wrap: wrap; margin-bottom: 20px; }
    .img-col { flex: 1 1 250px; text-align: center; }
    .caption { font-size: 0.9rem; color: #555; margin-top: 4px; }
  </style>
</head>
<body>
  <h1>CS 180 Project 5A – Diffusion Models</h1>
  <p><strong>Name:</strong> Mansoor Mamnoon</p>
  <p><strong>Random Seed:</strong> 180</p>

  <h2>Part 0: Playing with DeepFloyd IF</h2>

  <p>
    I used the DeepFloyd IF two-stage diffusion model via Hugging Face. I generated my own
    prompt embeddings using the provided Hugging Face cluster, then loaded the resulting
    <code>prompt_embeds_dict.pth</code> file into my environment. For all experiments below,
    I fixed the random seed to <code>180</code> for reproducibility.
  </p>

  <h3>Prompts</h3>
  <ol>
    <li>"A giant mechanical peacock made of bronze gears and stained glass, opening its tail in the middle of Times Square at night."</li>
    <li>"A Victorian-era astronaut reading a book inside a candle-lit library on the moon."</li>
    <li>"A warm, cozy cottage made out of chocolate and gingerbread, with smoke shaped like cinnamon rolls drifting from the chimney."</li>
  </ol>

  <h3>Generated Images (Stage II, 256×256, 20 steps)</h3>
  <div class="img-row">
    <div class="img-col">
      <img src="img/peacock_20.png" alt="Mechanical peacock, 20 steps">
      <div class="caption">Prompt 1 – 20 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/astronaut_20.png" alt="Victorian-era astronaut, 20 steps">
      <div class="caption">Prompt 2 – 20 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/cottage_20.png" alt="Chocolate gingerbread cottage, 20 steps">
      <div class="caption">Prompt 3 – 20 denoising steps</div>
    </div>
  </div>

  <h3>Reflection on Image Quality and Alignment</h3>

  <p><strong>Prompt 1 – Mechanical peacock in Times Square.</strong>
    The model produces a very vibrant peacock with a stained-glass-like tail and metallic-looking body,
    which matches the “mechanical” and “stained glass” parts of the prompt well. In the 256×256 image,
    the background has bright lights and tall buildings that resemble Times Square at night.
    The overall composition is coherent, but some of the gear details in the feathers are abstract
    rather than clearly defined gears.
  </p>

  <p><strong>Prompt 2 – Victorian-era astronaut in a candle-lit library on the moon.</strong>
    The image shows a figure in an old-fashioned uniform reading a large book in front of floor-to-ceiling shelves.
    The warm directional light fits the “candle-lit” setting. A planet visible through the window
    suggests a lunar location. The model captures the mood of a Victorian library well, but the “astronaut”
    concept is stylized and does not look like a modern spacesuit.
  </p>

  <p><strong>Prompt 3 – Chocolate and gingerbread cottage with cinnamon-roll smoke.</strong>
    The cottage clearly looks like a gingerbread house: the walls and roof have cookie and icing textures,
    and there are candy-like decorations and trees. The scene feels warm and cozy despite the snow.
    The smoke from the chimney has a slight spiral structure, but it does not perfectly form distinct
    cinnamon-roll shapes. Overall, the model is strong at the “dessert house” concept, but struggles with
    this very specific geometric detail.
  </p>

  <h3>Effect of <code>num_inference_steps</code> (Prompt 1)</h3>

  <div class="img-row">
    <div class="img-col">
      <img src="img/peacock_10steps.png" alt="Peacock, 10 steps">
      <div class="caption">Prompt 1 – 10 denoising steps</div>
    </div>
    <div class="img-col">
      <img src="img/peacock_50steps.png" alt="Peacock, 50 steps">
      <div class="caption">Prompt 1 – 50 denoising steps</div>
    </div>
  </div>

  <p>
    For this comparison I fixed the seed to 180 and only changed <code>num_inference_steps</code>.
    With <strong>10 steps</strong>, the global structure of the peacock is present, but the textures in the
    feathers and background are blurrier and contain more small artifacts. With <strong>50 steps</strong>,
    the same basic composition is preserved, but the feathers and stained-glass patterns are sharper and more
    detailed, and the city lights look more stable. This matches the intuition that more denoising steps let
    the diffusion process refine the image more thoroughly, trading extra compute time for better visual quality.
  </p>

  <h2>Part 1.1: Forward Diffusion Process</h2>

  <p>
    In this section, I visualize the <strong>forward process</strong> of diffusion on a real image.
    Starting from a clean 64×64 Campanile photo \(x_0\), I apply the forward noising equation
    \(
      x_t = \sqrt{\bar{\alpha}_t}\, x_0 + \sqrt{1 - \bar{\alpha}_t}\, \epsilon
    \)
    using the same \( \bar{\alpha}_t \) schedule as the DeepFloyd model. As the timestep
    <code>t</code> increases, the contribution of the original image decreases and the contribution of
    Gaussian noise increases.
  </p>

  <h3>Implementation: Forward Diffusion Function</h3>

  <p>
    I implement the forward diffusion step
    \(x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1 - \bar{\alpha}_t}\,\epsilon\)
    in a helper function <code>forward(im, t)</code>. Given a clean image
    \(x_0\) and a timestep <code>t</code>, it looks up \(\bar{\alpha}_t\) from
    the pretrained schedule <code>alphas_cumprod</code>, samples fresh Gaussian
    noise \(\epsilon \sim \mathcal{N}(0, I)\), and returns the corresponding
    noisy image \(x_t\).
  </p>

  <pre><code>def forward(im, t):
    """
    Args:
      im : torch tensor of size (1, 3, 64, 64) representing the clean image
      t  : integer timestep

    Returns:
      im_noisy : torch tensor of size (1, 3, 64, 64) representing the noisy
                 image at timestep t
    """
    with torch.no_grad():
        # Look up cumulative product of alphas at timestep t
        alpha_bar_t = alphas_cumprod[t].to(im.device)
        sqrt_alpha_bar_t = alpha_bar_t.sqrt()
        sqrt_one_minus_alpha_bar_t = (1.0 - alpha_bar_t).sqrt()

        # Sample Gaussian noise epsilon ~ N(0, I)
        eps = torch.randn_like(im)

        # Forward diffusion equation: x_t = sqrt(a_bar_t) * x_0
        #                            + sqrt(1 - a_bar_t) * eps
        im_noisy = sqrt_alpha_bar_t * im + sqrt_one_minus_alpha_bar_t * eps

    return im_noisy</code></pre>


  <h3>Campanile at Different Noise Levels</h3>

  <div class="img-row">
    <div class="img-col">
      <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
      <div class="caption">Campanile, clean image (t = 0)</div>
    </div>
    <div class="img-col">
      <img src="img/campanile_t250.png" alt="Noisy Campanile at t = 250">
      <div class="caption">Campanile at t = 250</div>
    </div>
  </div>

  <div class="img-row">
    <div class="img-col">
      <img src="img/campanile_t500.png" alt="Noisy Campanile at t = 500">
      <div class="caption">Campanile at t = 500</div>
    </div>
    <div class="img-col">
      <img src="img/campanile_t750.png" alt="Noisy Campanile at t = 750">
      <div class="caption">Campanile at t = 750</div>
    </div>
  </div>

  <h3>Discussion</h3>

  <p>
    At <strong>t = 0</strong>, the image is completely clean: we see a sharp Campanile with clear
    edges and textures. At <strong>t = 250</strong>, there is noticeable grain, but the tower and
    background are still clearly recognizable. By <strong>t = 500</strong>, the structure is much
    harder to see: the noise dominates and only a faint outline of the tower remains. At
    <strong>t = 750</strong>, the image is nearly pure noise, and most semantic content has been
    destroyed. This matches the design of the diffusion forward process: as t increases, the signal
    \(\sqrt{\bar{\alpha}_t} x_0\) shrinks, and the noise term \(\sqrt{1 - \bar{\alpha}_t} \epsilon\)
    grows, providing a smooth path from a real image to pure Gaussian noise.
  </p>

  <h2>Part 1.2: Classical Gaussian Denoising</h2>

<p>
  In this part, I tried to denoise the forward-diffused Campanile images from
  \(t = 250, 500, 750\) using a classical Gaussian blur filter. The idea is that
  averaging nearby pixels should reduce high-frequency noise. However, Gaussian
  blur does not know anything about the underlying image structure, so it
  inevitably removes edges and details along with the noise.
</p>

<h3>Noisy vs. Gaussian-Blurred Campanile</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t250.png" alt="Noisy Campanile at t = 250">
    <div class="caption">Noisy Campanile (t = 250)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gauss_t250.png" alt="Gaussian blur denoised Campanile at t = 250">
    <div class="caption">Gaussian blur denoised (t = 250)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t500.png" alt="Noisy Campanile at t = 500">
    <div class="caption">Noisy Campanile (t = 500)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gauss_t500.png" alt="Gaussian blur denoised Campanile at t = 500">
    <div class="caption">Gaussian blur denoised (t = 500)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_t750.png" alt="Noisy Campanile at t = 750">
    <div class="caption">Noisy Campanile (t = 750)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gauss_t750.png" alt="Gaussian blur denoised Campanile at t = 750">
    <div class="caption">Gaussian blur denoised (t = 750)</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  For <strong>t = 250</strong>, Gaussian blur slightly smooths the grain and makes the sky look
  cleaner, but it also softens the edges of the Campanile. At <strong>t = 500</strong>, the filter
  removes some high-frequency noise, yet the tower structure becomes very blurry and hard to see.
  By <strong>t = 750</strong>, almost all semantic content has already been destroyed by the forward
  process, and Gaussian blur can only smooth the noise into an even more featureless blob. This
  illustrates a key limitation of classical denoising: simple low-pass filters cannot reconstruct
  lost structure; they can only trade noise for sharpness.
</p>


<h2>Part 1.3: One-Step Denoising with the DeepFloyd UNet</h2>

<p>
  In this part, I use the pretrained DeepFloyd Stage&nbsp;1 UNet to perform
  <strong>one-step denoising</strong> on the noisy Campanile images from
  \(t = 250, 500, 750\). The UNet has been trained on a huge dataset of
  \((x_0, x_t, t)\) pairs to predict the Gaussian noise \(\epsilon\) that was
  added at each timestep. Given a noisy image \(x_t\), a timestep \(t\), and a
  text embedding (here I use the unconditional/null embedding), the model
  predicts \(\hat{\epsilon}\). Using the forward diffusion equation
  \[
    x_t = \sqrt{\bar{\alpha}_t}\, x_0 + \sqrt{1 - \bar{\alpha}_t}\, \epsilon,
  \]
  I invert it to recover an estimate of the original image:
  \[
    \hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t}\, \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}}.
  \]
</p>

<h3>Original vs Noisy vs One-Step Denoised</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t250.png" alt="Noisy Campanile at t = 250">
    <div class="caption">Noisy Campanile (t = 250)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_onestep_t250.png" alt="One-step denoised Campanile at t = 250">
    <div class="caption">One-step denoised (t = 250)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t500.png" alt="Noisy Campanile at t = 500">
    <div class="caption">Noisy Campanile (t = 500)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_onestep_t500.png" alt="One-step denoised Campanile at t = 500">
    <div class="caption">One-step denoised (t = 500)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean.png" alt="Clean Campanile image (t = 0)">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_t750.png" alt="Noisy Campanile at t = 750">
    <div class="caption">Noisy Campanile (t = 750)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_onestep_t750.png" alt="One-step denoised Campanile at t = 750">
    <div class="caption">One-step denoised (t = 750)</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  Compared to the <strong>Gaussian blur</strong> results in Part&nbsp;1.2, the learned UNet
  denoiser produces much sharper and more faithful reconstructions. At
  \(t = 250\), the one-step denoised image is very close to the original Campanile,
  with clear edges and recognizable structure. At \(t = 500\), some details are
  still recovered, but fine textures and contrast begin to wash out. By
  \(t = 750\), the model struggles: the forward process has already destroyed
  most semantic information, so the reconstruction looks more like a generic
  “building-like” blob than the true Campanile.
</p>

<p>
  This highlights a key advantage of diffusion models over classical filters:
  instead of blindly smoothing pixels, the UNet has learned a prior over natural
  images and uses both the noisy input and the timestep \(t\) to infer which
  structures are likely to be real signal and which are noise. However, it also
  shows a limitation of one-step denoising from very large \(t\): when the noise
  level is too high, the information about the original image is fundamentally
  ambiguous, and even a strong generative model cannot perfectly reconstruct it.
</p>

<h2>Part 1.4: Iterative Denoising</h2>

<p>
  In this section, I implement <strong>iterative denoising</strong> with DeepFloyd IF. Instead of
  attempting to recover the clean image <code>x_0</code> in a single step, I begin from a very
  noisy image at timestep <code>t = 690</code> (from a strided schedule
  <code>[990, 960, ..., 0]</code>) and repeatedly apply the DDPM update rule to move toward
  progressively smaller timesteps until reaching <code>t = 0</code>.
</p>

<p>
  At each step, the UNet predicts both the noise component and a learned variance term. I first
  estimate <code>x_0</code> from the current noisy image <code>x_t</code>, use this to construct
  the DDPM mean for the next timestep <code>x_{t'}</code>, and then add the correct amount of
  variance using the provided <code>add_variance</code> function. This produces a sequence of
  images that gradually increase signal-to-noise ratio while remaining consistent with the
  diffusion model’s training process.
</p>

<h3>Implementation: Iterative Denoising with Learned Variance</h3>

<p>
  To reverse the diffusion process, I repeatedly apply a DDPM-style update
  from a noisy image \(x_t\) to a slightly less noisy image \(x_{t'}\).
  The helper <code>add_variance</code> uses the scheduler’s learned variance
  prediction to add the correct amount of noise at each step, and
  <code>iterative_denoise</code> loops over all timesteps until reaching
  \(t = 0\).
</p>

<pre><code>def add_variance(predicted_variance, t, image):
  """
  Args:
    predicted_variance : (1, 3, 64, 64) tensor, last three channels of UNet output
    t                  : tensor indicating timestep
    image              : (1, 3, 64, 64) tensor, current image

  Returns:
    (1, 3, 64, 64) tensor, image with the correct amount of variance added
  """
  # Query the DDPM scheduler for the variance at timestep t
  variance = stage_1.scheduler._get_variance(
      t, predicted_variance=predicted_variance
  )
  variance_noise = torch.randn_like(image)
  variance = torch.exp(0.5 * variance) * variance_noise
  return image + variance


def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
  """
  Perform iterative denoising from timestep timesteps[i_start] down to 0.

  Args:
    im_noisy      : (1, 3, 64, 64) noisy image at timesteps[i_start]
    i_start       : index into `timesteps` to start from
    prompt_embeds : text embedding used to condition the UNet
    timesteps     : list of integer timesteps, e.g. [990, 960, ..., 0]
    display       : if True, show intermediate results every few steps

  Returns:
    clean : numpy array of shape (1, 3, 64, 64) in [-1, 1]
  """
  image = im_noisy  # x_t

  with torch.no_grad():
      for i in range(i_start, len(timesteps) - 1):
          # 1) Move from timestep t to the next smaller timestep prev_t
          t = timesteps[i]
          prev_t = timesteps[i + 1]

          # 2) Load alpha-bar values and compute alpha, beta
          alpha_bar_t = alphas_cumprod[t].to(image.device).type_as(image)
          alpha_bar_prev = alphas_cumprod[prev_t].to(image.device).type_as(image)

          alpha = alpha_bar_t / alpha_bar_prev
          beta = 1.0 - alpha

          # 3) UNet predicts noise and variance at timestep t
          model_output = stage_1.unet(
              image,
              t,
              encoder_hidden_states=prompt_embeds,
              return_dict=False
          )[0]  # (1, 6, 64, 64)

          noise_est, predicted_variance = torch.split(
              model_output, image.shape[1], dim=1
          )

          # 4) Estimate x_0 from x_t and predicted noise
          sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
          sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

          x0_hat = (image - sqrt_one_minus_alpha_bar_t * noise_est) / sqrt_alpha_bar_t

          # 5) Compute DDPM mean for x_{t'}
          coef_x0 = torch.sqrt(alpha_bar_prev) * beta / (1.0 - alpha_bar_t)
          coef_xt = torch.sqrt(alpha) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)

          pred_prev_image_mean = coef_x0 * x0_hat + coef_xt * image

          # 6) Add learned variance on the scheduler's device
          scheduler_device = stage_1.scheduler.timesteps.device
          scheduler_dtype = stage_1.scheduler.timesteps.dtype

          pred_prev_for_sched = pred_prev_image_mean.to(
              scheduler_device, dtype=torch.float32
          )
          predicted_variance_for_sched = predicted_variance.to(
              scheduler_device, dtype=torch.float32
          )
          t_tensor = torch.tensor([t], device=scheduler_device, dtype=scheduler_dtype)

          pred_prev_with_var = add_variance(
              predicted_variance_for_sched,
              t_tensor,
              pred_prev_for_sched,
          )

          # 7) Move back to model device and precision
          image = pred_prev_with_var.to(im_noisy.device).type_as(im_noisy)

          if display and ((i - i_start) % 5 == 0 or i == len(timesteps) - 2):
              print(f"Denoising step {i - i_start}: t = {t} -> t' = {prev_t}")
              img_vis = image[0].detach().cpu().permute(1, 2, 0) / 2.0 + 0.5
              media.show_image(img_vis)

      clean = image.cpu().detach().numpy()

  return clean</code></pre>


<h3>Noisy Campanile at Selected Timesteps</h3>

<p>
  Below are snapshots of the Campanile after the forward diffusion process at several timesteps
  in my strided schedule. These correspond to intermediate states that the iterative denoiser
  must reverse:
</p>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_noisy_t690.png" alt="Noisy Campanile at t = 690">
    <div class="caption">Noisy Campanile (t = 690)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_noisy_t540.png" alt="Noisy Campanile at t = 540">
    <div class="caption">Noisy Campanile (t = 540)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_noisy_t390.png" alt="Noisy Campanile at t = 390">
    <div class="caption">Noisy Campanile (t = 390)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_noisy_t240.png" alt="Noisy Campanile at t = 240">
    <div class="caption">Noisy Campanile (t = 240)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_noisy_t90.png" alt="Noisy Campanile at t = 90">
    <div class="caption">Noisy Campanile (t = 90)</div>
  </div>
</div>

<h3>Iterative vs One-Step vs Gaussian Denoising</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_clean_t0.png" alt="Original Campanile, clean image">
    <div class="caption">Original Campanile (t = 0)</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_iter_denoised.png" alt="Iteratively denoised Campanile">
    <div class="caption">Iteratively denoised (start t = 690)</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/campanile_one_step.png" alt="One-step denoised Campanile">
    <div class="caption">One-step denoised from t = 690</div>
  </div>
  <div class="img-col">
    <img src="img/campanile_gaussian_blur.png" alt="Gaussian blurred Campanile">
    <div class="caption">Gaussian blur (kernel 5, σ = 2)</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  Starting from very heavy noise (<code>t = 690</code>), the
  <strong>one-step</strong> denoising estimate actually preserves the
  <em>structure</em> of the Campanile surprisingly well: the tower shape and
  the original top are still recognizable, even though the image is quite
  noisy and low quality overall. This makes sense, because we only apply the
  UNet once to directly invert the noisy sample <code>x_t</code>, without
  repeatedly resampling new noise.
</p>

<p>
  In contrast, the <strong>iterative</strong> denoising procedure produces a
  much cleaner and sharper image, but it sometimes
  <strong>hallucinates a different tower top</strong> than in the original
  photograph. By the time we reach <code>t = 690</code> in the forward
  process, most of the original signal has been destroyed, so the model is
  effectively “dreaming” a plausible tall tower rather than reconstructing
  the exact Campanile. Repeatedly estimating <code>\hat{x}_0</code>,
  resampling, and adding variance allows the sampler to move along the
  diffusion model’s natural image manifold, which improves realism but can
  drift away from the exact ground-truth details.
</p>

<p>
  Finally, the <strong>Gaussian blur</strong> baseline is smooth but
  fundamentally limited: it only removes high-frequency noise and cannot
  recover missing structure. Compared to it, diffusion-based denoising (both
  one-step and iterative) is able to restore meaningful geometry and texture,
  highlighting the advantage of learning a generative model of natural
  images rather than relying on purely classical filtering.
</p>


<hr>

<h2>Part 1.5: Diffusion Model Sampling</h2>

<p>
  In this section, I use my <code>iterative_denoise</code> function not to clean up an existing
  image, but to <strong>generate images from pure noise</strong>. Following the assignment,
  I sample an initial tensor
  \(x_T \sim \mathcal{N}(0, I)\) of shape \(1 \times 3 \times 64 \times 64\) and then run the
  denoising loop from the noisiest timestep down to \(t = 0\) using the same
  <code>strided_timesteps</code> schedule as in Part&nbsp;1.4.
</p>

<p>
  The denoiser is conditioned on the text prompt embedding for
  <code>"a high quality photo"</code>. At each timestep \(t\), the UNet predicts the noise in the
  current image, which I combine with the diffusion coefficients
  \(\bar{\alpha}_t\) to form an estimate of the clean image \(x_0\). I then use the DDPM update
  rule with learned variance to step from \(x_t\) to a slightly less noisy image \(x_{t'}\),
  and repeat until I reach a final sample at \(t = 0\).
</p>

<h3>Samples from Pure Noise (Prompt: "a high quality photo")</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/part1_5_sample_1.png" alt="Sample 1 from pure noise">
    <div class="caption">Sample 1</div>
  </div>
  <div class="img-col">
    <img src="img/part1_5_sample_2.png" alt="Sample 2 from pure noise">
    <div class="caption">Sample 2</div>
  </div>
  <div class="img-col">
    <img src="img/part1_5_sample_3.png" alt="Sample 3 from pure noise">
    <div class="caption">Sample 3</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/part1_5_sample_4.png" alt="Sample 4 from pure noise">
    <div class="caption">Sample 4</div>
  </div>
  <div class="img-col">
    <img src="img/part1_5_sample_5.png" alt="Sample 5 from pure noise">
    <div class="caption">Sample 5</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  All five images start from independent Gaussian noise but share the same text prompt
  <code>"a high quality photo"</code>. Even without classifier-free guidance, the model produces
  reasonably realistic, photo-like outputs: we see natural textures, lighting, and depth,
  rather than pure noise. However, the samples are still somewhat soft or painterly and can
  contain artifacts and odd details. This matches the expectation from the assignment:
  the iterative denoising procedure steers noise towards the natural image manifold, but
  without stronger guidance (which I add later with CFG), the images are only loosely
  constrained by the prompt.
</p>


<h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>

<p>
  In Part 1.5, I sampled images by denoising pure Gaussian noise using my
  iterative diffusion sampler. The results looked vaguely photographic, but
  many images were blurry or lacked clear structure. In this section I use
  <strong>Classifier-Free Guidance (CFG)</strong> to improve sample quality.
</p>

<p>
  At each denoising step, the UNet predicts two noise estimates:
  a <em>conditional</em> noise estimate \(\epsilon_c\) given the text prompt
  "<code>a high quality photo</code>", and an <em>unconditional</em> noise
  estimate \(\epsilon_u\) given the empty prompt "".
  CFG combines them as:
</p>

<p style="text-align:center;">
  \[
    \epsilon = \epsilon_u + \gamma \, (\epsilon_c - \epsilon_u),
  \]
</p>

<p>
  where \(\gamma\) is the guidance scale. I use \(\gamma = 7\). When
  \(\gamma = 0\), the model ignores the text and behaves unconditionally;
  when \(\gamma = 1\), it uses the normal conditional prediction; and when
  \(\gamma &gt; 1\), it exaggerates the difference between conditional and
  unconditional noise, which tends to produce sharper, more on-prompt images
  at the cost of diversity.
</p>

<h3>Implementation: Iterative Denoising with Classifier-Free Guidance</h3>

<p>
  For Classifier-Free Guidance (CFG), I run the UNet twice at each timestep:
  once with the conditional prompt embedding \(\epsilon_c\) for
  "<code>a high quality photo</code>", and once with the unconditional embedding
  \(\epsilon_u\) for the empty prompt. I combine them as
  \(\epsilon = \epsilon_u + \gamma (\epsilon_c - \epsilon_u)\) with
  guidance scale \(\gamma = 7\), and then use this guided noise in the same
  DDPM update rule as in Part&nbsp;1.4.
</p>

<pre><code># Precomputed text embeddings
prompt_embeds = prompt_embeds_dict["a high quality photo"]
uncond_prompt_embeds = prompt_embeds_dict[""]


def iterative_denoise_cfg(
  im_noisy,
  i_start,
  prompt_embeds,
  uncond_prompt_embeds,
  timesteps,
  scale=7,
  display=True,
):
  """
  Iterative denoising with Classifier-Free Guidance (CFG).

  Args:
    im_noisy            : (1, 3, 64, 64) noisy image at timesteps[i_start]
    i_start             : index into `timesteps` to start from
    prompt_embeds       : conditional text embedding
    uncond_prompt_embeds: unconditional (empty) text embedding
    timesteps           : list of timesteps, e.g. [990, 960, ..., 0]
    scale               : CFG guidance scale gamma
    display             : if True, show intermediate images

  Returns:
    clean : numpy array of shape (1, 3, 64, 64) in [-1, 1]
  """
  image = im_noisy

  with torch.no_grad():
      for i in range(i_start, len(timesteps) - 1):
          t = timesteps[i]
          prev_t = timesteps[i + 1]

          # 1) Load alpha-bar values and compute alpha, beta
          alpha_bar_t = alphas_cumprod[t].to(image.device).type_as(image)
          alpha_bar_prev = alphas_cumprod[prev_t].to(image.device).type_as(image)

          alpha = alpha_bar_t / alpha_bar_prev
          beta = 1.0 - alpha

          # 2) Conditional UNet pass (with text)
          model_output = stage_1.unet(
              image,
              t,
              encoder_hidden_states=prompt_embeds,
              return_dict=False
          )[0]

          # 3) Unconditional UNet pass (empty prompt)
          uncond_output = stage_1.unet(
              image,
              t,
              encoder_hidden_states=uncond_prompt_embeds,
              return_dict=False
          )[0]

          # Split into noise and variance
          noise_est, predicted_variance = torch.split(
              model_output, image.shape[1], dim=1
          )
          uncond_noise_est, _ = torch.split(
              uncond_output, image.shape[1], dim=1
          )

          # 4) Classifier-Free Guidance:
          #    eps = eps_u + scale * (eps_c - eps_u)
          guided_noise = uncond_noise_est + scale * (noise_est - uncond_noise_est)

          # 5) Estimate x_0 from x_t and guided noise
          sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
          sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

          x0_hat = (image - sqrt_one_minus_alpha_bar_t * guided_noise) / sqrt_alpha_bar_t

          # 6) DDPM mean for x_{t'}
          coef_x0 = torch.sqrt(alpha_bar_prev) * beta / (1.0 - alpha_bar_t)
          coef_xt = torch.sqrt(alpha) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)

          pred_prev_image_mean = coef_x0 * x0_hat + coef_xt * image

          # 7) Add learned variance via scheduler
          scheduler_device = stage_1.scheduler.timesteps.device
          scheduler_dtype = stage_1.scheduler.timesteps.dtype

          pred_prev_for_sched = pred_prev_image_mean.to(
              scheduler_device, dtype=torch.float32
          )
          predicted_variance_for_sched = predicted_variance.to(
              scheduler_device, dtype=torch.float32
          )
          t_tensor = torch.tensor([t], device=scheduler_device, dtype=scheduler_dtype)

          pred_prev_with_var = add_variance(
              predicted_variance_for_sched,
              t_tensor,
              pred_prev_for_sched,
          )

          # 8) Move back to model device / dtype
          image = pred_prev_with_var.to(im_noisy.device).type_as(im_noisy)

          if display and ((i - i_start) % 5 == 0 or i == len(timesteps) - 2):
              print(f"[CFG] Denoising step {i - i_start}: t = {t} -> t' = {prev_t}")
              img_vis = image[0].detach().cpu().permute(1, 2, 0) / 2.0 + 0.5
              media.show_image(img_vis)

      clean = image.cpu().detach().numpy()

  return clean</code></pre>


<h3>CFG Samples for "a high quality photo"</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/cfg_sample_1.png" alt="CFG sample 1: portrait-style photo">
    <div class="caption">Sample 1 with CFG (\(\gamma = 7\))</div>
  </div>
  <div class="img-col">
    <img src="img/cfg_sample_2.png" alt="CFG sample 2: portrait-style photo">
    <div class="caption">Sample 2 with CFG</div>
  </div>
  <div class="img-col">
    <img src="img/cfg_sample_3.png" alt="CFG sample 3: portrait-style photo">
    <div class="caption">Sample 3 with CFG</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/cfg_sample_4.png" alt="CFG sample 4: portrait-style photo">
    <div class="caption">Sample 4 with CFG</div>
  </div>
  <div class="img-col">
    <img src="img/cfg_sample_5.png" alt="CFG sample 5: portrait-style photo">
    <div class="caption">Sample 5 with CFG</div>
  </div>
</div>

<h3>Discussion</h3>

<p>
  All samples use the same random seed setup as earlier parts and are generated
  by starting from pure Gaussian noise and running my
  <code>iterative_denoise_cfg</code> loop with the prompt
  "<code>a high quality photo</code>" and guidance scale \(\gamma = 7\).
  Compared to the unguided samples in Part 1.5, these images are noticeably
  more <strong>photorealistic</strong>: they look like well-lit portraits with
  sharper edges, clearer facial features, and more consistent lighting and
  background structure. At the same time, the samples are less diverse: most
  images collapse toward a similar “high-quality portrait” style, which
  illustrates the usual tradeoff of CFG – higher fidelity but reduced variety.
</p>


<h2>Part 1.7: Image-to-image Translation (SDEdit)</h2>

<p>
  In this part I reuse my iterative denoising sampler, but instead of starting from pure noise
  I start from a <em>noised real image</em>. I first apply the forward diffusion process to the
  original 64×64 Campanile image to obtain \(x_t\) at various timesteps, and then run
  <code>iterative_denoise_cfg</code> from that point down to \(t = 0\) using the prompt
  "<code>a high quality photo</code>". This is essentially the SDEdit algorithm: it projects
  a noisy image back onto the natural image manifold, with the amount of change controlled by
  how much noise I add.
</p>

<h3>1.7 – Campanile SDEdit (noise levels 1, 3, 5, 7, 10, 20)</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_campanile_i1.png" alt="Campanile SDEdit i_start=1">
    <div class="caption">Campanile SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i3.png" alt="Campanile SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i5.png" alt="Campanile SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_campanile_i7.png" alt="Campanile SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i10.png" alt="Campanile SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_campanile_i20.png" alt="Campanile SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_campanile_original.png" alt="Original Campanile">
    <div class="caption">Original Campanile</div>
  </div>
</div>

<p>
  For very small noise (i_start = 1, 3), the edits are subtle: the sky and trees shift slightly
  but the tower is almost unchanged. As I start from larger noise levels (i_start = 10, 20), the
  model has to hallucinate much more content, so the campanile’s top and background can change
  substantially while still staying “photo-like”.
</p>

<h3>1.7 – SDEdit on My Own Photos (House and Beach)</h3>

<p>
  I also applied the same procedure to two web images: a coastal town with white houses and red
  roofs, and a beach scene with a hat and sandals. In each case I ran SDEdit from noise levels
  [1, 3, 5, 7, 10, 20] with the generic prompt "<code>a high quality photo</code>".
</p>

<h4>Coastal Town</h4>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_i1.png" alt="House SDEdit i_start=1">
    <div class="caption">House SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i3.png" alt="House SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i5.png" alt="House SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_i7.png" alt="House SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i10.png" alt="House SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_house_i20.png" alt="House SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_original.png" alt="Original house photo">
    <div class="caption">Original coastal town photo</div>
  </div>
</div>

<h4>Beach Scene</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_i1.png" alt="Beach SDEdit i_start=1">
    <div class="caption">Beach SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i3.png" alt="Beach SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i5.png" alt="Beach SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_i7.png" alt="Beach SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i10.png" alt="Beach SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_beach_i20.png" alt="Beach SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_original.png" alt="Original beach photo">
    <div class="caption">Original beach photo</div>
  </div>
</div>

<h3>1.7.1 Editing Hand-Drawn and Web Images</h3>

<p>
  SDEdit is especially fun when I start from non-realistic inputs such as clipart and my own
  sketches. Below I show one web image (a vector-style avocado) and two hand-drawn images that I
  drew in the Colab canvas, each projected to the natural image manifold at increasing noise
  levels.
</p>

<h4>Web Image SDEdit</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_web_i1.png" alt="Web SDEdit i_start=1">
    <div class="caption">Web SDEdit (i_start = 1)</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i3.png" alt="Web SDEdit i_start=3">
    <div class="caption">i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i5.png" alt="Web SDEdit i_start=5">
    <div class="caption">i_start = 5</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_web_i7.png" alt="Web SDEdit i_start=7">
    <div class="caption">i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i10.png" alt="Web SDEdit i_start=10">
    <div class="caption">i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_web_i20.png" alt="Web SDEdit i_start=20">
    <div class="caption">i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_web_original.png" alt="Original web image">
    <div class="caption">Original web image</div>
  </div>
</div>

<h4>Two Hand-Drawn Inputs</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand1_i1.png" alt="Hand-drawn 1 SDEdit i_start=1">
    <div class="caption">Hand-drawn #1, i_start = 1</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i3.png" alt="Hand-drawn 1 SDEdit i_start=3">
    <div class="caption">Hand-drawn #1, i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i5.png" alt="Hand-drawn 1 SDEdit i_start=5">
    <div class="caption">Hand-drawn #1, i_start = 5</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i7.png" alt="Hand-drawn 1 SDEdit i_start=7">
    <div class="caption">Hand-drawn #1, i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i10.png" alt="Hand-drawn 1 SDEdit i_start=10">
    <div class="caption">Hand-drawn #1, i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand1_i20.png" alt="Hand-drawn 1 SDEdit i_start=20">
    <div class="caption">Hand-drawn #1, i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand1_original.png" alt="Original hand-drawn 1">
    <div class="caption">Original hand-drawn #1</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand2_i1.png" alt="Hand-drawn 2 SDEdit i_start=1">
    <div class="caption">Hand-drawn #2, i_start = 1</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i3.png" alt="Hand-drawn 2 SDEdit i_start=3">
    <div class="caption">Hand-drawn #2, i_start = 3</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i5.png" alt="Hand-drawn 2 SDEdit i_start=5">
    <div class="caption">Hand-drawn #2, i_start = 5</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i7.png" alt="Hand-drawn 2 SDEdit i_start=7">
    <div class="caption">Hand-drawn #2, i_start = 7</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i10.png" alt="Hand-drawn 2 SDEdit i_start=10">
    <div class="caption">Hand-drawn #2, i_start = 10</div>
  </div>
  <div class="img-col">
    <img src="img/sedit_hand2_i20.png" alt="Hand-drawn 2 SDEdit i_start=20">
    <div class="caption">Hand-drawn #2, i_start = 20</div>
  </div>
</div>

<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_hand2_original.png" alt="Original hand-drawn 2">
    <div class="caption">Original hand-drawn #2</div>
  </div>
</div>

<h3>1.7.2 Inpainting</h3>

<p>
  For inpainting, I modify my CFG denoising loop so that at every timestep I overwrite the
  unmasked region with a noised version of the original image \(x_{\text{orig}}\) at the same
  timestep. If \(m\) is a binary mask where 1 indicates the editable region, the update is
</p>

<p style="text-align:center;">
  \(x_t \leftarrow m \odot x_t + (1 - m)\odot \text{forward}(x_{\text{orig}}, t).\)
</p>

<p>
  This keeps everything outside the mask consistent with the original photo, while the diffusion
  model freely resynthesizes content inside the mask.
</p>

<h4>Code: Inpainting Function</h4>

<p>
  Below is the core PyTorch implementation of my <code>inpaint</code> function. It reuses the
  CFG denoising loop from earlier parts, but at every iteration it overwrites the unmasked
  region with a noised version of the original image so that only the masked pixels are edited.
</p>

<pre><code>def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds,
          timesteps, scale=7, display=False):
  """
  original_image : (1, 3, H, W) in [-1, 1]
  mask           : (1, 3, H, W), 1 = editable region, 0 = keep original
  prompt_embeds  : conditional text embedding
  uncond_prompt_embeds : unconditional (empty) text embedding
  timesteps      : diffusion schedule (e.g. strided_timesteps)
  scale          : classifier-free guidance scale
  """

  # Work on GPU, keep original separate
  original_image = original_image.to(device).float()
  x = torch.randn_like(original_image, device=device, dtype=torch.float16)

  mask = mask.to(device).type_as(x)
  prompt_embeds        = prompt_embeds.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

  with torch.no_grad():
      for i in range(len(timesteps) - 1):
          t = timesteps[i]
          t_prev = timesteps[i + 1]

          # 1) Force unmasked region to follow the original image at timestep t
          noisy_orig = forward(original_image, t).to(device).type_as(x)
          x = mask * x + (1.0 - mask) * noisy_orig

          # 2) Compute noise schedule coefficients
          alpha_bar_t   = alphas_cumprod[t].to(device).type_as(x)
          alpha_bar_prev = alphas_cumprod[t_prev].to(device).type_as(x)
          alpha = alpha_bar_t / alpha_bar_prev
          beta  = 1.0 - alpha

          sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
          sqrt_one_minus_alpha_bar_t = torch.sqrt(1.0 - alpha_bar_t)

          # 3) Conditional and unconditional UNet passes
          cond_out = stage_1.unet(x, t, encoder_hidden_states=prompt_embeds,
                                  return_dict=False)[0]
          uncond_out = stage_1.unet(x, t, encoder_hidden_states=uncond_prompt_embeds,
                                    return_dict=False)[0]

          eps_c, pred_var = torch.split(cond_out, x.shape[1], dim=1)
          eps_u, _        = torch.split(uncond_out, x.shape[1], dim=1)

          # 4) Classifier-free guidance
          eps = eps_u + scale * (eps_c - eps_u)

          # 5) Estimate x0 and DDPM mean for the previous timestep
          x0_hat = (x - sqrt_one_minus_alpha_bar_t * eps) / sqrt_alpha_bar_t

          coef_x0 = torch.sqrt(alpha_bar_prev) * beta / (1.0 - alpha_bar_t)
          coef_xt = torch.sqrt(alpha) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)
          mean = coef_x0 * x0_hat + coef_xt * x

          # 6) Add learned variance
          t_tensor = torch.tensor([t], device=stage_1.scheduler.timesteps.device,
                                  dtype=stage_1.scheduler.timesteps.dtype)
          mean_for_sched = mean.to(torch.float32)
          var_for_sched  = pred_var.to(torch.float32)

          x = add_variance(var_for_sched, t_tensor, mean_for_sched)
          x = x.to(device).type_as(mask)

  return x.cpu().numpy()</code></pre>


<h4>Campanile Inpainting</h4>

<div class="img-row">
  <div class="img-col">
    <img src="img/inpaint_campanile_original.png" alt="Campanile original">
    <div class="caption">Original Campanile</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_campanile_mask.png" alt="Campanile inpainting mask">
    <div class="caption">Inpainting mask (top of tower)</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_campanile_result.png" alt="Campanile inpainted">
    <div class="caption">Inpainted Campanile</div>
  </div>
</div>

<p>
  The model fills the top of the tower with a plausible structure that matches the lighting and
  perspective of the original image, even though that region was completely resampled.
</p>

<h4>More Inpainting Examples</h4>

<p>
  Here I show three more masks on two different images. Each triplet shows the original image,
  the binary mask, and the final inpainted result.
</p>

<h5>House – Circular Roof Mask</h5>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_original.png" alt="House original">
    <div class="caption">Original house</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_circle_mask.png" alt="House circular mask">
    <div class="caption">Circular roof mask</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_circle_result.png" alt="House circular inpaint">
    <div class="caption">Inpainted house (circular)</div>
  </div>
</div>

<h5>House – Diamond Roof Mask</h5>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_house_original.png" alt="House original">
    <div class="caption">Original house</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_diamond_mask.png" alt="House diamond mask">
    <div class="caption">Diamond-shaped roof mask</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_house_diamond_result.png" alt="House diamond inpaint">
    <div class="caption">Inpainted house (diamond)</div>
  </div>
</div>

<h5>Beach – Vertical Band Mask</h5>
<div class="img-row">
  <div class="img-col">
    <img src="img/sedit_beach_original.png" alt="Beach original">
    <div class="caption">Original beach</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_beach_vertical_mask.png" alt="Beach vertical mask">
    <div class="caption">Vertical band mask</div>
  </div>
  <div class="img-col">
    <img src="img/inpaint_beach_vertical_result.png" alt="Beach vertical inpaint">
    <div class="caption">Inpainted beach (vertical band)</div>
  </div>
</div>


<!-- ============================= -->
<!--       1.7.3 SDEdit Section    -->
<!-- ============================= -->

<section id="sedit">
  <h1>1.7.3 Text-Conditioned Image-to-Image Translation</h1>

  <p>
    In this section, I use SDEdit with Classifier-Free Guidance to transform real images
    toward imaginative text prompts. Each transformation begins by adding noise at a given level
    (<code>i_start ∈ {1,3,5,7,10,20}</code>) and then denoising using a text prompt.
    Lower noise levels preserve the original structure, while higher noise levels result
    in stronger transformations.
  </p>

  <!-- ------------------------ -->
  <!-- CAMPANILE EXAMPLE        -->
  <!-- ------------------------ -->
  <h2>Campanile → “Futuristic Rocket Launch Tower”</h2>
  <p>
    <strong>Prompt:</strong>
    <em>a futuristic rocket launch tower standing on an alien planet with purple sky</em>
  </p>

  <div class="image-grid">
    <figure>
      <img src="img/campanile_sedit_rocket_1.png" alt="Campanile i_start 1">
      <figcaption>i_start = 1</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_3.png" alt="Campanile i_start 3">
      <figcaption>i_start = 3</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_5.png" alt="Campanile i_start 5">
      <figcaption>i_start = 5</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_7.png" alt="Campanile i_start 7">
      <figcaption>i_start = 7</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_10.png" alt="Campanile i_start 10">
      <figcaption>i_start = 10</figcaption>
    </figure>
    <figure>
      <img src="img/campanile_sedit_rocket_20.png" alt="Campanile i_start 20">
      <figcaption>i_start = 20</figcaption>
    </figure>
  </div>

  <figure>
    <img src="img/campanile_clean.png" alt="Original Campanile" class="original-img">
    <figcaption>Original Campanile Image</figcaption>
  </figure>



  <!-- ------------------------ -->
  <!-- BEACH EXAMPLE            -->
  <!-- ------------------------ -->
  <h2>Beach → “Penguins Sliding Down Icy Hills”</h2>
  <p>
    <strong>Prompt:</strong>
    <em>a snowy mountain landscape with penguins sliding down icy hills</em>
  </p>

  <div class="image-grid">
    <figure>
      <img src="img/beach_sedit_penguins_1.png">
      <figcaption>i_start = 1</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_3.png">
      <figcaption>i_start = 3</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_5.png">
      <figcaption>i_start = 5</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_7.png">
      <figcaption>i_start = 7</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_10.png">
      <figcaption>i_start = 10</figcaption>
    </figure>
    <figure>
      <img src="img/beach_sedit_penguins_20.png">
      <figcaption>i_start = 20</figcaption>
    </figure>
  </div>

  <figure>
    <img src="img/sedit_beach_original.png" class="original-img">
    <figcaption>Original Beach Image</figcaption>
  </figure>



  <!-- ------------------------ -->
  <!-- HOUSE EXAMPLE            -->
  <!-- ------------------------ -->
  <h2>Coastal Town → “Glowing Mushroom Village”</h2>
  <p>
    <strong>Prompt:</strong>
    <em>a medieval village built entirely out of enormous glowing mushrooms</em>
  </p>

  <div class="image-grid">
    <figure>
      <img src="img/house_sedit_mushrooms_1.png">
      <figcaption>i_start = 1</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_3.png">
      <figcaption>i_start = 3</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_5.png">
      <figcaption>i_start = 5</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_7.png">
      <figcaption>i_start = 7</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_10.png">
      <figcaption>i_start = 10</figcaption>
    </figure>
    <figure>
      <img src="img/house_sedit_mushrooms_20.png">
      <figcaption>i_start = 20</figcaption>
    </figure>
  </div>

  <figure>
    <img src="img/sedit_house_original.png" class="original-img">
    <figcaption>Original House Image</figcaption>
  </figure>

</section>


<!-- ============================= -->
<!-- CSS for Grid Layout           -->
<!-- ============================= -->

<style>
  #sedit {
    font-family: Arial, sans-serif;
    margin-bottom: 60px;
  }

  .image-grid {
    display: grid;
    grid-template-columns: repeat(3, 1fr);
    gap: 18px;
    margin: 20px 0;
  }

  .image-grid figure {
    text-align: center;
  }

  .image-grid img {
    width: 100%;
    border-radius: 6px;
    border: 1px solid #ccc;
  }

  .original-img {
    width: 40%;
    display: block;
    margin: 25px auto;
    border-radius: 6px;
    border: 1px solid #ccc;
  }

  figcaption {
    margin-top: 6px;
    font-size: 0.9rem;
    color: #555;
  }
</style>


<h2>Part 1.8: Visual Anagrams</h2>

<p>
  In this part I implement <strong>visual anagrams</strong>: single images that look like one
  subject when upright but reveal a different subject when flipped upside down. At each
  denoising step I compute two guided noise estimates:
</p>

<p style="text-align:center;">
  \(\epsilon_1 = \text{CFG}(\text{UNet}(x_t, t, p_1))\) <br>
  \(\epsilon_2 = \text{flip}\big(\text{CFG}(\text{UNet}(\text{flip}(x_t), t, p_2))\big)\)
</p>

<p>
  where \(p_1\) and \(p_2\) are two different prompts, and <code>flip</code> is a vertical flip.
  I then average them:
</p>

<p style="text-align:center;">
  \(\epsilon = (\epsilon_1 + \epsilon_2) / 2\),
</p>

<p>
  and use \(\epsilon\) in the DDPM update. The same underlying latents are constrained to be
  compatible with both prompts, but in opposite orientations.
</p>

<h4>Code: Visual Anagram Sampler</h4>

<p>
  The function below implements the dual-path denoising loop for visual anagrams. At each
  timestep it runs the UNet on the current image with prompt \(p_1\), and on a vertically
  flipped copy with prompt \(p_2\). The second noise estimate is flipped back and averaged with
  the first before applying the DDPM update.
</p>

<pre><code>def visual_anagram(i_start,
                 prompt_embeds_1,
                 prompt_embeds_2,
                 uncond_prompt_embeds,
                 timesteps,
                 scale=7,
                 display=False):
  """
  Generate a visual anagram:
    upright image matches prompt p1,
    upside-down image matches prompt p2.
  """

  x = torch.randn(1, 3, 64, 64, device=device, dtype=torch.float16)

  prompt_embeds_1 = prompt_embeds_1.to(device).half()
  prompt_embeds_2 = prompt_embeds_2.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

  with torch.no_grad():
      for i in range(i_start, len(timesteps) - 1):
          t = timesteps[i]
          t_prev = timesteps[i + 1]

          # Noise schedule
          alpha_bar_t   = alphas_cumprod[t].to(device).type_as(x)
          alpha_bar_prev = alphas_cumprod[t_prev].to(device).type_as(x)
          alpha = alpha_bar_t / alpha_bar_prev
          beta  = 1.0 - alpha

          sqrt_alpha_bar_t = torch.sqrt(alpha_bar_t)
          sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar_t)

          # Path 1: upright image, prompt p1
          out1 = stage_1.unet(x, t, encoder_hidden_states=prompt_embeds_1,
                              return_dict=False)[0]
          uncond1 = stage_1.unet(x, t, encoder_hidden_states=uncond_prompt_embeds,
                                 return_dict=False)[0]
          eps_c1, pred_var = torch.split(out1, x.shape[1], dim=1)
          eps_u1, _        = torch.split(uncond1, x.shape[1], dim=1)
          eps1 = eps_u1 + scale * (eps_c1 - eps_u1)

          # Path 2: flipped image, prompt p2
          x_flip = torch.flip(x, dims=[2])  # flip vertically
          out2 = stage_1.unet(x_flip, t, encoder_hidden_states=prompt_embeds_2,
                              return_dict=False)[0]
          uncond2 = stage_1.unet(x_flip, t, encoder_hidden_states=uncond_prompt_embeds,
                                 return_dict=False)[0]
          eps_c2, _ = torch.split(out2, x.shape[1], dim=1)
          eps_u2, _ = torch.split(uncond2, x.shape[1], dim=1)
          eps2 = eps_u2 + scale * (eps_c2 - eps_u2)
          eps2 = torch.flip(eps2, dims=[2])  # flip noise back

          # Average the two guided noises
          eps = 0.5 * (eps1 + eps2)

          # DDPM update x_t -> x_{t_prev}
          x0_hat = (x - sqrt_one_minus_alpha_bar * eps) / sqrt_alpha_bar_t

          coef_x0 = torch.sqrt(alpha_bar_prev) * beta / (1.0 - alpha_bar_t)
          coef_xt = torch.sqrt(alpha) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)
          mean = coef_x0 * x0_hat + coef_xt * x

          t_tensor = torch.tensor([t], device=stage_1.scheduler.timesteps.device,
                                  dtype=stage_1.scheduler.timesteps.dtype)
          x = add_variance(pred_var.to(torch.float32), t_tensor, mean.to(torch.float32))

  return x.cpu().numpy()</code></pre>


<h3>Old Man / Campfire Illusion</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/anagram_oldman_upright.png" alt="Anagram old man upright">
    <div class="caption">Upright: "an oil painting of an old man"</div>
  </div>
  <div class="img-col">
    <img src="img/anagram_oldman_flipped.png" alt="Anagram campfire flipped">
    <div class="caption">Flipped: "an oil painting of people around a campfire"</div>
  </div>
</div>

<p>
  Upright, the image reads as a portrait of an older man with strong facial shadows. When flipped,
  those shadows line up into a circle of figures and a glowing light source, resembling a campfire
  scene.
</p>

<h3>Cat Drawing / Vine Skull Illusion</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/anagram_cat_upright.png" alt="Anagram cat upright">
    <div class="caption">Upright: "a detailed pencil drawing of a cat"</div>
  </div>
  <div class="img-col">
    <img src="img/anagram_cat_flipped.png" alt="Anagram skull flipped">
    <div class="caption">Flipped: "a spooky human skull made of vines"</div>
  </div>
</div>

<p>
  In this illusion, the fur and whiskers of the upright cat become tangled vine structures and eye
  sockets when flipped, forming the skull.
</p>

<h3>Castle / Cloud Dragon Illusion</h3>

<div class="img-row">
  <div class="img-col">
    <img src="img/anagram_castle_upright.png" alt="Anagram castle upright">
    <div class="caption">Upright: "a fantasy castle on a hill at sunset"</div>
  </div>
  <div class="img-col">
    <img src="img/anagram_castle_flipped.png" alt="Anagram cloud dragon flipped">
    <div class="caption">Flipped: "a dragon's head made of clouds"</div>
  </div>
</div>

<p>
  Here the castle silhouette and clouds double as the jawline and horns of a dragon when viewed
  upside down. In all three cases, the shared latent image must satisfy two different prompts at
  once, which the CFG-based dual-path denoising makes possible.
</p>

<h2>Part 1.9: Hybrid Images with Factorized Diffusion</h2>

<p>
  Finally, I implement <strong>hybrid images</strong> using Factorized Diffusion. At each
  timestep I compute two guided noise predictions:
</p>

<p style="text-align:center;">
  \(\epsilon_1 = \text{CFG}(\text{UNet}(x_t, t, p_1)),\quad
    \epsilon_2 = \text{CFG}(\text{UNet}(x_t, t, p_2))\).
</p>

<p>
  I then take low frequencies from \(\epsilon_1\) and high frequencies from \(\epsilon_2\):
</p>

<p style="text-align:center;">
  \(\epsilon = f_{\text{lowpass}}(\epsilon_1) + f_{\text{highpass}}(\epsilon_2),\)
</p>

<p>
  where <code>f_lowpass</code> is implemented with a Gaussian blur (kernel size 33, σ = 2) and
  <code>f_highpass(x) = x - f_lowpass(x)</code>. This composite noise drives the denoising
  process so that the final image looks like prompt \(p_1\) when blurred or viewed from far away,
  but like prompt \(p_2\) when viewed up close.
</p>

<h4>Code: Hybrid Image Sampler</h4>

<p>
  The <code>make_hybrids</code> function below combines low frequencies from one guided noise
  estimate and high frequencies from another using a Gaussian blur in noise space. The resulting
  composite noise is then used in the DDPM update at every timestep.
</p>

<pre><code>import torchvision.transforms.functional as TF

def make_hybrids(i_start,
               prompt_embeds_1,   # low-frequency prompt
               prompt_embeds_2,   # high-frequency prompt
               uncond_prompt_embeds,
               timesteps,
               scale=7,
               display=False):
  """
  Hybrid sampling:
    eps1 = CFG(UNet(x_t, t, p1))
    eps2 = CFG(UNet(x_t, t, p2))
    eps  = lowpass(eps1) + highpass(eps2)
  """

  device = next(stage_1.unet.parameters()).device
  stage_1.scheduler.set_timesteps(timesteps=timesteps)

  x = torch.randn(1, 3, 64, 64, device=device, dtype=torch.float16)

  prompt_embeds_1 = prompt_embeds_1.to(device).half()
  prompt_embeds_2 = prompt_embeds_2.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

  kernel_size = 33
  sigma = 2.0

  with torch.no_grad():
      for i in range(i_start, len(timesteps) - 1):
          t = timesteps[i]
          t_prev = timesteps[i + 1]

          alpha_bar_t   = alphas_cumprod[t].to(device).type_as(x)
          alpha_bar_prev = alphas_cumprod[t_prev].to(device).type_as(x)
          alpha = alpha_bar_t / alpha_bar_prev
          beta  = 1.0 - alpha

          # Conditional and unconditional UNet passes
          out1 = stage_1.unet(x, t, encoder_hidden_states=prompt_embeds_1,
                              return_dict=False)[0]
          out2 = stage_1.unet(x, t, encoder_hidden_states=prompt_embeds_2,
                              return_dict=False)[0]
          outu = stage_1.unet(x, t, encoder_hidden_states=uncond_prompt_embeds,
                              return_dict=False)[0]

          eps1_raw, pred_var = torch.split(out1, x.shape[1], dim=1)
          eps2_raw, _        = torch.split(out2, x.shape[1], dim=1)
          eps_u, _           = torch.split(outu, x.shape[1], dim=1)

          # CFG noises
          eps1 = eps_u + scale * (eps1_raw - eps_u)
          eps2 = eps_u + scale * (eps2_raw - eps_u)

          # Frequency splitting in noise space
          eps1_f = eps1.float()
          eps2_f = eps2.float()

          eps1_low = TF.gaussian_blur(eps1_f, kernel_size=kernel_size, sigma=sigma)
          eps2_low = TF.gaussian_blur(eps2_f, kernel_size=kernel_size, sigma=sigma)
          eps2_high = eps2_f - eps2_low

          eps = eps1_low + eps2_high

          # (Optional) renormalize hybrid noise to reasonable variance
          target_std = (eps1_f.std() + eps2_f.std()) / 2.0
          eps = eps - eps.mean()
          eps = eps * (target_std / (eps.std() + 1e-6))
          eps = eps.to(x.dtype)

          # DDPM step
          sqrt_alpha_bar_t   = torch.sqrt(alpha_bar_t)
          sqrt_one_minus_abt = torch.sqrt(1.0 - alpha_bar_t)

          x0_hat = (x - sqrt_one_minus_abt * eps) / sqrt_alpha_bar_t

          coef_x0 = torch.sqrt(alpha_bar_prev) * beta / (1.0 - alpha_bar_t)
          coef_xt = torch.sqrt(alpha) * (1.0 - alpha_bar_prev) / (1.0 - alpha_bar_t)
          mean = coef_x0 * x0_hat + coef_xt * x

          t_tensor = torch.tensor([t], device=stage_1.scheduler.timesteps.device,
                                  dtype=stage_1.scheduler.timesteps.dtype)
          x = add_variance(pred_var.to(torch.float32), t_tensor, mean.to(torch.float32))

  return x.cpu().numpy()</code></pre>


  <h3>Hybrid Image Results</h3>

  <p>
    Below are all of the hybrid images I generated using the factorized diffusion
    sampler. In each case, the low frequencies come from a “soft” or blurry prompt
    (background / global structure), while the high frequencies come from a detailed,
    high-contrast prompt (edges and fine details).
  </p>

  <div class="img-row">
    <div class="img-col">
      <img src="img/hybrid_skull_waterfall.png" alt="Hybrid of stone skull and waterfall">
      <div class="caption">
        Hybrid 1 – Low frequencies: “a detailed photo of a waterfall in a lush green forest”;
        High frequencies: “a large stone skull on a dark background”.
      </div>
    </div>
    <div class="img-col">
      <img src="img/hybrid_nebula_owl.png" alt="Hybrid of nebula galaxy and owl drawing">
      <div class="caption">
        Hybrid 2 – Low frequencies: “a colorful nebula galaxy with soft glowing clouds”;
        High frequencies: “a detailed pencil drawing of an owl’s face”.
      </div>
    </div>
  </div>

  <div class="img-row">
    <div class="img-col">
      <img src="img/hybrid_sunset_city.png" alt="Hybrid of sunset and city skyline drawing">
      <div class="caption">
        Hybrid 3 – Low frequencies: “a soft blurry sunset over the ocean with warm colors”;
        High frequencies: “a detailed black and white line drawing of a city skyline”.
      </div>
    </div>
    <div class="img-col">
      <img src="img/hybrid_face_skull.png" alt="Hybrid of portrait and skull drawing">
      <div class="caption">
        Hybrid 4 – Low frequencies: “a soft focus color portrait of a woman smiling”;
        High frequencies: “a detailed black and white pencil drawing of a human skull”.
      </div>
    </div>
  </div>


<p>
  At low spatial frequencies the image is dominated by smooth glowing nebula blobs, so it looks
  like a colorful galaxy. At high spatial frequencies, the pencil-like lines that form the owl’s
  eyes and feathers become visible. The hybrid effectively hides a sketch inside a soft
  astronomical background.
</p>

<p>
  Overall, visual anagrams and hybrid images highlight how flexible the diffusion framework is:
  by manipulating the noise estimates at each step (e.g., flipping, frequency splitting), I can
  steer the model to satisfy multiple competing constraints at different spatial scales or
  orientations.
</p>










</body>
</html>
